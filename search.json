[{"title":"机器学习+深度学习量化","url":"/2025/03/08/机器学习+深度学习量化/","content":"\n### 背景\n最近在港股美股上的收益还可以，但是在择时能力上感觉靠人的判断没有特别的准确，希望通过量化的方式去帮助择时，也不希望做高频，只希望高抛低吸做t的时候可以更准确\n\n### 调研\n一开始尝试了bigquant，发现需要开会员才能用到比较高级的因子和策略，所以找了下开源的量化回测框架和数据\n目前采用的是以下几个库：\n1. [talib](https://github.com/TA-Lib/ta-lib-python:)\n2. [backtrader](https://github.com/mementum/backtrader)\n3. [akshare](https://github.com/akfamily/akshare)\n4. [pybroker](https://github.com/edtechre/pybroker)\n\n目前做到的是用机器学习和深度学习来做未来涨跌的判定，dnn/gnn的情况还在研究，因为构造数据集和回测框架的兼容上比机器学习难度大一些；\n\n### 效果\n特征粗略制作，样本就挑选了一个a股为例\n#### 交易图表：\n![汇总结论](https://github.com/miaohancheng/picx-images-hosting/raw/master/pics/backtrader_plot.7axbcmuk0o.webp)\n\n#### 执行过程\n1. 模型训练+分步减仓\n![减仓](https://github.com/miaohancheng/picx-images-hosting/raw/master/pics/执行过程.7snd18d7o4.webp)\n2. 模型训练+止损\n![止损](https://github.com/miaohancheng/picx-images-hosting/raw/master/pics/止损.26lmndbwuu.webp)\n\n#### 数据表现：\n- 最终资金: 1,483,439.88 CNY （初始资金：1,000,000.00 CNY）\n- 回测时间：2018-01-01 至 2025-03-01 的 大A\n- 年化夏普比率: 0.51\n- 最大回撤: 30.80% (持续 489 天)\n- 年化收益率: 5.90%（只能跑赢通胀）\n\n### 代码思路：\n初始化引入：\n```python\nimport os\nimport warnings\nimport logging\nimport akshare as ak\nimport backtrader as bt\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport talib as ta\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import StandardScaler\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n\n# ----------------------- 日志设置与警告屏蔽 -----------------------\nDEBUG = False\nif DEBUG:\n    logging.basicConfig(level=logging.DEBUG, format='%(asctime)s %(levelname)s: %(message)s')\nelse:\n    logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s')\n\nlogger = logging.getLogger(__name__)\n\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n```\n\n\n#### 模型构建:\n1. transfromer\n```python\n## ----------------------- Transformer模型定义 -----------------------\nclass TransformerClassifier(nn.Module):\n    def __init__(self, input_dim, nhead=2, num_encoder_layers=2, dim_feedforward=64, dropout=0.1):\n        super(TransformerClassifier, self).__init__()\n        # 如果 input_dim 不能被 nhead 整除，则增加一个线性层映射到新维度\n        if input_dim % nhead != 0:\n            new_input_dim = ((input_dim // nhead) + 1) * nhead\n            self.input_proj = nn.Linear(input_dim, new_input_dim)\n            d_model = new_input_dim\n        else:\n            self.input_proj = None\n            d_model = input_dim\n\n        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,\n                                                   dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n        # 分类层\n        self.fc = nn.Linear(d_model, 1)\n\n    def forward(self, x):\n        # x: (batch_size, seq_length, input_dim)\n        if self.input_proj is not None:\n            x = self.input_proj(x)\n        transformer_out = self.transformer_encoder(x)  # (batch_size, seq_length, d_model)\n        pooled = transformer_out.mean(dim=1)             # (batch_size, d_model)\n        output = self.fc(pooled)\n        return torch.sigmoid(output)\n\n```\n\n2. Bilstm+Attention\n```python\n\n# ------------------ 定义 BiLSTM+Attention 模型 ------------------\nclass BiLSTMAttention(nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_layers=1):\n        super(BiLSTMAttention, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True)\n        # 注意力层：将每个时刻的输出映射为一个得分\n        self.attn_layer = nn.Linear(hidden_dim * 2, 1)\n        self.fc = nn.Linear(hidden_dim * 2, 1)\n\n    def forward(self, x):\n        # x: (batch_size, seq_length, input_dim)\n        lstm_out, _ = self.lstm(x)  # (batch_size, seq_length, hidden_dim*2)\n        attn_scores = self.attn_layer(lstm_out)  # (batch_size, seq_length, 1)\n        attn_weights = torch.softmax(attn_scores, dim=1)  # (batch_size, seq_length, 1)\n        context = torch.sum(attn_weights * lstm_out, dim=1)  # (batch_size, hidden_dim*2)\n        output = self.fc(context)  # (batch_size, 1)\n        output = torch.sigmoid(output)\n        return output\n```\n\n#### 模型训练\n1. transformer\n```python\n\ndef train_transformer_model(X_train_seq, y_train_seq, X_val_seq, y_val_seq, input_dim,\n                            nhead=2, num_encoder_layers=2, dim_feedforward=64, dropout=0.1,\n                            epochs=20, batch_size=32, patience=5):\n    X_train_tensor = torch.tensor(X_train_seq, dtype=torch.float32)\n    y_train_tensor = torch.tensor(y_train_seq, dtype=torch.float32).unsqueeze(1)\n    X_val_tensor = torch.tensor(X_val_seq, dtype=torch.float32)\n    y_val_tensor = torch.tensor(y_val_seq, dtype=torch.float32).unsqueeze(1)\n\n    model = TransformerClassifier(input_dim, nhead, num_encoder_layers, dim_feedforward, dropout)\n    criterion = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    best_val_loss = float('inf')\n    epochs_no_improve = 0\n    best_model_state = None\n    for epoch in range(epochs):\n        model.train()\n        permutation = torch.randperm(X_train_tensor.size(0))\n        train_loss = 0\n        for i in range(0, X_train_tensor.size(0), batch_size):\n            optimizer.zero_grad()\n            indices = permutation[i:i+batch_size]\n            batch_x = X_train_tensor[indices]\n            batch_y = y_train_tensor[indices]\n            outputs = model(batch_x)\n            loss = criterion(outputs, batch_y)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item() * batch_x.size(0)\n        train_loss /= X_train_tensor.size(0)\n\n        model.eval()\n        with torch.no_grad():\n            val_outputs = model(X_val_tensor)\n            val_loss = criterion(val_outputs, y_val_tensor).item()\n\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            best_model_state = model.state_dict()\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n            if epochs_no_improve >= patience:\n                break\n    if best_model_state is not None:\n        model.load_state_dict(best_model_state)\n    model.eval()\n    with torch.no_grad():\n        val_preds = model(X_val_tensor)\n        val_preds_binary = (val_preds >= 0.5).float()\n        accuracy = (val_preds_binary.eq(y_val_tensor).sum().item() / y_val_tensor.size(0))\n    return model, accuracy\n\n```\n\n2. BiLSTM+Attention\n```python\ndef train_bilstm_attention_model(X_train_seq, y_train_seq, X_val_seq, y_val_seq, input_dim,\n                                 hidden_dim=32, epochs=20, batch_size=32, patience=5):\n    X_train_tensor = torch.tensor(X_train_seq, dtype=torch.float32)\n    y_train_tensor = torch.tensor(y_train_seq, dtype=torch.float32).unsqueeze(1)\n    X_val_tensor = torch.tensor(X_val_seq, dtype=torch.float32)\n    y_val_tensor = torch.tensor(y_val_seq, dtype=torch.float32).unsqueeze(1)\n\n    model = BiLSTMAttention(input_dim, hidden_dim)\n    criterion = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    best_val_loss = float('inf')\n    epochs_no_improve = 0\n    best_model_state = None\n    for epoch in range(epochs):\n        model.train()\n        permutation = torch.randperm(X_train_tensor.size(0))\n        train_loss = 0\n        for i in range(0, X_train_tensor.size(0), batch_size):\n            optimizer.zero_grad()\n            indices = permutation[i:i+batch_size]\n            batch_x = X_train_tensor[indices]\n            batch_y = y_train_tensor[indices]\n            outputs = model(batch_x)\n            loss = criterion(outputs, batch_y)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item() * batch_x.size(0)\n        train_loss /= X_train_tensor.size(0)\n\n        model.eval()\n        with torch.no_grad():\n            val_outputs = model(X_val_tensor)\n            val_loss = criterion(val_outputs, y_val_tensor).item()\n\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            best_model_state = model.state_dict()\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n            if epochs_no_improve >= patience:\n                break\n    if best_model_state is not None:\n        model.load_state_dict(best_model_state)\n    model.eval()\n    with torch.no_grad():\n        val_preds = model(X_val_tensor)\n        val_preds_binary = (val_preds >= 0.5).float()\n        accuracy = (val_preds_binary.eq(y_val_tensor).sum().item() / y_val_tensor.size(0))\n    return model, accuracy\n```\n\n3. 辅助函数\n```python\n# 辅助函数：构造序列数据\ndef create_sequences(X, y, seq_length):\n    X_seq, y_seq = [], []\n    for i in range(len(X) - seq_length + 1):\n        X_seq.append(X[i:i+seq_length])\n        y_seq.append(y[i+seq_length-1])\n    return np.array(X_seq), np.array(y_seq)\n```\n\n#### 特征工程：\n```python\n# ----------------------- 特征工程模块 -----------------------\ndef compute_features(close_list, volume_list, prediction_window):\n    \"\"\"\n    构造特征数据，返回DataFrame，其中：\n      - close_list: 按时间顺序排列的收盘价列表（最早在前）\n      - volume_list: 成交量列表\n      - prediction_window: 预测窗口（未来N日）\n    \"\"\"\n    df = pd.DataFrame({'close': close_list, 'volume': volume_list})\n    df['high'] = df['close']\n    df['low'] = df['close']\n\n    # 1. 滞后收益率\n    for window in [5, 10, 20]:\n        df[f'rtn_lag_{window}'] = df['close'].pct_change(window)\n\n    # 2. 日收益率 & 波动率\n    df['daily_return'] = df['close'].pct_change()\n    df['volatility_10'] = df['daily_return'].rolling(window=10).std()\n\n    # 3. 均线及差值\n    df['ma_20'] = df['close'].rolling(window=20).mean()\n    df['ma_diff'] = df['close'] / (df['ma_20'] + 1e-8) - 1\n\n    # 4. RSI（自定义与TA版本）\n    delta = df['close'].diff()\n    gain = delta.clip(lower=0)\n    loss = -delta.clip(upper=0)\n    avg_gain = gain.rolling(14).mean()\n    avg_loss = loss.rolling(14).mean()\n    rs = avg_gain / (avg_loss + 1e-8)\n    df['rsi_14_custom'] = 100 - (100 / (1 + rs))\n    df['ta_rsi'] = ta.RSI(df['close'].values, timeperiod=14)\n\n    # 5. MACD\n    df['ema_12'] = df['close'].ewm(span=12, adjust=False).mean()\n    df['ema_26'] = df['close'].ewm(span=26, adjust=False).mean()\n    df['macd_custom'] = df['ema_12'] - df['ema_26']\n    df['macd_signal_custom'] = df['macd_custom'].ewm(span=9, adjust=False).mean()\n    df['macd_hist_custom'] = df['macd_custom'] - df['macd_signal_custom']\n    df['ta_macd'], df['ta_macd_signal'], df['ta_macd_hist'] = ta.MACD(\n        df['close'].values, fastperiod=12, slowperiod=26, signalperiod=9)\n\n    # 6. Bollinger Bands\n    df['ta_boll_upper'], df['ta_boll_middle'], df['ta_boll_lower'] = ta.BBANDS(\n        df['close'].values, timeperiod=20)\n\n    # 7. ADX\n    df['ta_adx'] = ta.ADX(df['high'].values, df['low'].values, df['close'].values, timeperiod=14)\n\n    # 8. OBV\n    df['ta_obv'] = ta.OBV(df['close'].values, df['volume'].values)\n\n    # 9. 慢速KD\n    slowk, slowd = ta.STOCH(\n        df['high'].values, df['low'].values, df['close'].values,\n        fastk_period=14, slowk_period=3, slowk_matype=0,\n        slowd_period=3, slowd_matype=0\n    )\n    df['ta_stoch_slowk'] = slowk\n    df['ta_stoch_slowd'] = slowd\n\n    # 10. CCI\n    df['ta_cci'] = ta.CCI(df['high'].values, df['low'].values, df['close'].values, timeperiod=14)\n\n    # 11. Momentum\n    df['ta_mom'] = ta.MOM(df['close'].values, timeperiod=10)\n\n    # 12. ATR\n    df['ta_atr'] = ta.ATR(df['high'].values, df['low'].values, df['close'].values, timeperiod=14)\n\n    # 13. MFI\n    df['ta_mfi'] = ta.MFI(df['high'].values, df['low'].values, df['close'].values,\n                          df['volume'].values, timeperiod=14)\n\n    # 14. PPO\n    df['ta_ppo'] = ta.PPO(df['close'].values, fastperiod=12, slowperiod=26, matype=0)\n\n    # 15. ROC\n    df['ta_roc'] = ta.ROC(df['close'].values, timeperiod=10)\n\n    # 16. TRIX\n    df['ta_trix'] = ta.TRIX(df['close'].values, timeperiod=15)\n\n    # 17. Williams %R\n    df['ta_willr'] = ta.WILLR(df['high'].values, df['low'].values, df['close'].values, timeperiod=14)\n\n    # 18. StochRSI\n    df['ta_stochrsi'] = ta.STOCHRSI(\n        df['close'].values, timeperiod=14,\n        fastk_period=3, fastd_period=3, fastd_matype=0\n    )[0]\n\n    # 19. ULTOSC\n    df['ta_ultosc'] = ta.ULTOSC(\n        df['high'].values, df['low'].values, df['close'].values,\n        timeperiod1=7, timeperiod2=14, timeperiod3=28\n    )\n\n    # 20. Ichimoku\n    def compute_ichimoku(high, low):\n        high_series = pd.Series(high)\n        low_series = pd.Series(low)\n        tenkan = (high_series.rolling(window=9).max() + low_series.rolling(window=9).min()) / 2\n        kijun = (high_series.rolling(window=26).max() + low_series.rolling(window=26).min()) / 2\n        senkou_span_a = ((tenkan + kijun) / 2).shift(26)\n        senkou_span_b = (high_series.rolling(window=52).max() + low_series.rolling(window=52).min()) / 2\n        senkou_span_b = senkou_span_b.shift(26)\n        return tenkan, kijun, senkou_span_a, senkou_span_b\n\n    tenkan, kijun, senkou_a, senkou_b = compute_ichimoku(df['high'].values, df['low'].values)\n    df['ichi_tenkan'] = tenkan\n    df['ichi_kijun'] = kijun\n    df['ichi_senkou_a'] = senkou_a\n    df['ichi_senkou_b'] = senkou_b\n\n    # 21. 未来N日累积涨跌幅（预测目标）\n    df['cum_return_future'] = df['close'].pct_change(periods=prediction_window).shift(-prediction_window)\n\n    # 22. 52周range\n    df['52w_high'] = df['close'].rolling(window=252).max()\n    df['52w_low'] = df['close'].rolling(window=252).min()\n    df['range_52w'] = (df['close'] - df['52w_low']) / (df['52w_high'] - df['52w_low'] + 1e-8)\n\n    # 23. PE (示例)\n    df['pe'] = 15\n\n    # 24. 换手率\n    df['vol_ma_20'] = df['volume'].rolling(window=20).mean()\n    df['turnover'] = df['volume'] / (df['vol_ma_20'] + 1e-8)\n\n    # 标签：未来累计涨幅超过1%为正样本\n    df['label'] = (df['cum_return_future'] > 0.01).astype(int)\n\n    df = df.fillna(method='ffill').fillna(0)\n\n    feature_cols = [col for col in df.columns if col not in ['label', 'cum_return_future', 'daily_return']]\n    df = df[feature_cols + ['label', 'cum_return_future', 'daily_return']]\n    df = df.fillna(method='ffill').fillna(0)\n    return df, feature_cols\n\n\n```\n特征上用到都是比较基础统计类的特征，时序相关的还没开，实际上如果开发时序类特征，lstm+transformer的效果会好很多\n\n#### 主程序\n策略上就是止盈止损+一些回撤止盈，比较简单就不写了，然后下面是代码主入口\n```python\n\n# ----------------------- 主程序 -----------------------\nif __name__ == '__main__':\n    cerebro = bt.Cerebro()\n\n    symbol = '600519'\n    stock_data = fetch_akshare_data(\n        symbol=symbol,\n        start_date=pd.to_datetime('2018-01-01'),\n        end_date=pd.to_datetime('2025-03-01')\n    )\n\n    logger.info(\"\\n[数据样例]\")\n    logger.info(stock_data.head(3))\n    logger.info(f\"\\n数据时间范围: {stock_data.index.min()} ~ {stock_data.index.max()}\")\n\n    data = bt.feeds.PandasData(\n        dataname=stock_data,\n        open=0, high=1, low=2, close=3, volume=4, openinterest=-1\n    )\n    cerebro.adddata(data)\n\n    cerebro.addstrategy(\n        MLStrategy,\n        training_period=504,\n        prediction_window=3,\n        probability_threshold=0.65,\n    )\n\n    cerebro.broker.setcash(1_000_000)\n    cerebro.broker.setcommission(commission=0.001)\n\n    cerebro.addanalyzer(bt.analyzers.PyFolio, _name='pyfolio')\n    cerebro.addanalyzer(bt.analyzers.SharpeRatio, riskfreerate=0.0, annualize=True,\n                        timeframe=bt.TimeFrame.Days, _name='sharpe')\n    cerebro.addanalyzer(bt.analyzers.DrawDown, _name='drawdown')\n    cerebro.addanalyzer(bt.analyzers.TradeAnalyzer, _name='trade_analyzer')\n    cerebro.addanalyzer(bt.analyzers.TimeReturn, timeframe=bt.TimeFrame.Days, _name='time_return')\n\n    logger.info(\"\\n启动回测...\")\n    results = cerebro.run()\n    strat = results[0]\n\n    final_value = cerebro.broker.getvalue()\n    logger.info(f\"\\n最终资金: {final_value:,.2f} CNY\")\n\n    sharpe = strat.analyzers.sharpe.get_analysis()\n    if sharpe.get('sharperatio') is not None:\n        logger.info(f\"年化夏普比率: {sharpe['sharperatio']:.2f}\")\n    else:\n        logger.info(\"年化夏普比率: N/A\")\n\n    drawdown = strat.analyzers.drawdown.get_analysis()\n    logger.info(f\"最大回撤: {drawdown['max']['drawdown']:.2f}% (持续 {drawdown['max']['len']} 天)\")\n\n    trade_analyzer = strat.analyzers.trade_analyzer.get_analysis()\n    logger.info(\"\\n交易统计: %s\", trade_analyzer)\n\n    time_return = strat.analyzers.time_return.get_analysis()\n    if len(time_return) > 0:\n        daily_returns = pd.Series(time_return)\n        total_ret = (daily_returns + 1.0).prod() - 1.0\n        days = len(daily_returns)\n        annual_ret = (1 + total_ret)**(252/days) - 1 if days > 0 else None\n        if annual_ret is not None:\n            logger.info(f\"年化收益率: {annual_ret:.2%}\")\n        else:\n            logger.info(\"无法计算年化收益率\")\n    else:\n        logger.info(\"time_return结果为空，无法计算年化收益率\")\n\n    figs = cerebro.plot(style='candlestick', volume=False, barup='red', bardown='green')\n    if figs and figs[0]:\n        fig = figs[0][0]\n        fig.savefig(\"backtrader_plot.png\")\n        logger.info(\"图形已保存为 backtrader_plot.png\")\n    plt.show()\n```\n### 下一步计划\n1. 构造特征要加强，要筛选出更多的因子，这次只是实验，所以因子挑选的比较简单\n2. 模型结构可以调整一下，深度和注意力可以加多\n3. gnn再尝试一下，板块内部的股票实际上是有关联的\n\n当然这个代码非常粗糙，仅作为学习记录，如果有建议和不足请大家指正","tags":["deep learning","python","machine learning","quant trading"],"categories":["技术文章"]},{"title":"美国绿卡流程记录","url":"/2024/12/01/美国绿卡流程记录/","content":"\n### 摘要\n\n主要介绍一下我在国内申请NIW，从开始准备材料到美国移民局通过审核的所有流程，现在排期还未排到，如果排到之后再更新面试、拿卡等流程。\n\n\n\n### 起因\n\n主要是因为covid-19的封控问题，导致想换个国家生活工作一段时间，但是已经错过了留学留在国外工作的时间，而美国公司直接招聘海外员工发放h1b的情况又比较少；\n\n其实可行的路就剩下两条：\n\n1. 国内直接找到可以rebase到海外的外企，比如google、amd这些\n2. 国内直接申请绿卡，拿到卡之后出去\n\n\n\n### 准备申请\n\n#### 前期调研\n\n首先查询了相关案例，发现有NIW、EB1A 两条路可以直接人在美国境外申请绿卡（无需雇主担保），两个要求、排期各不相同，两种类型具体介绍如下：\n\n- [NIW](https://zhuanlan.zhihu.com/p/652960567?zpf=1695417255872118784) (这个介绍也算是广告，大家自行甄别)\n\n- [EB1A](https://zhuanlan.zhihu.com/p/578761321)(这个介绍也算是广告，大家自行甄别)\n\n\n\n一开始是准备DIY，然后找了一个美国的移民律师咨询了NIW、EB1A等情况，咨询费200美元/小时；\n\n发现自己的情况在NIW的条件下勉强及格，EB1A的话目前还不达标，所以放弃了EB1A，专心搞NIW；\n\n\n\n#### 寻找中介\n\n我和律师发现自己申请材料的缺陷主要是两点，一个是美国相关的专业人士推荐信不够；另外是行业内的影响力不够，这点靠自己DIY比较困难，所以还是想了下决定找中介来做这个项目；\n\n知乎、搜索之类的加了好几个中介，发了多份材料之后，选定了一个中介来帮我找到律师写整个申请材料；\n\n其中自己要准备的材料非常多，下面列举一些：\n\n1. 薪资收入证明\n2. 在职证明\n3. 学历信息证明\n4. 专利信息\n5. 各种行业相关证书\n6. 新闻稿和发表论文等\n\n\n\n整体准备材料到律师完成，递交给美国移民局 耗时：10个月，其中因为疫情上海封控了一段时间，一般也是需要半年左右，很多材料可能手头都没有，还要去找和去学校、公司开具；\n\n\n\n### 移民局审核\n\n在律师完成所有的文件翻译、审核、校对之后，要求付清翻译费之类的费用，会直接帮我递交材料\n\n递交时间：2023-4\n\n<img src=\"https://github.com/miaohancheng/picx-images-hosting/raw/master/pics/niw-recipet.wilm8fufg.jpg\" alt=\"递件凭证\" style=\"zoom:33%;\" />\n\n\n\n中介反馈一般审批时间是半年左右\n\n等到了2023-9，收到了获批函，证明我的NIW申请第一步已经基本完成，后续等待排期；\n\n\n\n### 等待排期\n\n等待排期是比较漫长的，主要是没什么进展，一年可能就前进6个月左右；\n\n现在每晚1年申请绿卡，就要晚2年拿到。\n\n查看排期的比较好用的网站推荐：https://visa.careerengine.us/ （可以自己定好自己的pd）\n\n![排期表](https://github.com/miaohancheng/picx-images-hosting/raw/master/pics/排期表.73tzmehatm.jpg)\n\n\n\n\n\n### 待更新后续\n\n\n\n### 总结\n\n这个总体项目还是比较长的，建议大家如果有打算要尽早行动，早做准备。\n\n同时也是耗时耗精力，还会有很多意外、补充材料的情况。\n","tags":["NIW","美国绿卡"],"categories":["留学移民"]},{"title":"多语言NER模型微调","url":"/2024/11/29/多语言NER模型微调/","content":"\n\n\n### 背景\n\n工作中有遇到多语言的地址、短句等数据，需要标注出其中人名、快递公司、电话等信息，现有的开源数据集中这部分数据较少，主要问题是要自己构建对应的数据集、以及对于开源通过wiki训练的模型要尽可能保留训练的经验\n\n### 模型选取\n\n选择了RoBERTa基于 [**wikiann**](https://huggingface.co/datasets/unimelb-nlp/wikiann ) 预训练的模型 [**roberta-ner-multilingual**](https://huggingface.co/julian-schelb/roberta-ner-multilingual )，对于现实中的知识已经有一部分理解，但是对于地址等信息知识较少需要额外训练；\n\n最原始的基座模型是facebook的 [**xlm-roberta-large**](https://huggingface.co/FacebookAI/xlm-roberta-large)\n\n### 数据准备\n\n准备了训练和测试数据集放在了 `./data`目录下，数据大致格式如下\n\n```json\nJohn    B-PER\nlives   O\nin      O\nBerlin  B-LOC\n.      O\n\n他      O\n住      O\n在      O\n北京    B-LOC\n。      O\n```\n\n通过不同的标签定义不同的实体，在模型训练中可以通过单一语言的样本迁移一部分知识到其他语言，这也是多语言模型的一个优势。\n\n\n\n### 模型加载\n\n因为我在本机mac上训练，后续迁移到服务器上，所以写了判断设备的代码\n\n```python\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\nfrom transformers import DataCollatorForTokenClassification\nfrom datasets import Dataset, DatasetDict\nimport numpy as np\nimport evaluate\nfrom torch import nn\nimport torch.nn.functional as F\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# 确定设备\nif torch.cuda.is_available():\n    device = torch.device('cuda')\nelif torch.backends.mps.is_available():\n    device = torch.device('mps')\nelse:\n    device = torch.device('cpu')\n\n# 加载分词器和模型\ntokenizer = AutoTokenizer.from_pretrained(\"julian-schelb/roberta-ner-multilingual\", add_prefix_space=True)\nmodel = AutoModelForTokenClassification.from_pretrained(\"julian-schelb/roberta-ner-multilingual\")\nmodel.to(device)  # 将模型移动到设备\n\n```\n\n\n\n### 增加标签\n\n原有的模型中标签类别较少，如果有新的标签可以手动添加\n\n```python\n# 如果有新标签，更新标签列表和映射\noriginal_labels = list(model.config.id2label.values())\nnew_labels = ['B-FACILITY','I-FACILITY' ] # 加入你新的标签即可\nlabel_list = original_labels + new_labels\nlabel_to_id = {label: idx for idx, label in enumerate(label_list)}\nid_to_label = {idx: label for idx, label in enumerate(label_list)}\n```\n\n\n\n更新模型配置\n\n```python\n# 更新模型配置\nmodel.config.num_labels = len(label_list)\nmodel.config.id2label = id_to_label\nmodel.config.label2id = label_to_id\n\n# 替换分类层\nmodel.classifier = nn.Linear(model.config.hidden_size, model.config.num_labels)\n```\n\n\n\n### 教师模型\n\n希望模型在学习新知识的前提下，不要遗忘旧知识，所以再增加一个教师模型来做监督\n\n```python\n# 加载教师模型\nteacher_model = AutoModelForTokenClassification.from_pretrained(\"julian-schelb/roberta-ner-multilingual\")\nteacher_model.to(device)  # 将教师模型移动到设备\nteacher_model.eval()\n\n# 可选：冻结学生模型的部分层\nfor name, param in model.named_parameters():\n    if name.startswith('roberta.embeddings') or name.startswith('roberta.encoder.layer.0') or name.startswith('roberta.encoder.layer.1'):\n        param.requires_grad = False\n\n```\n\n\n\n### 加载数据\n\n```python\n# 定义读取数据的函数\ndef read_conll_data(filename):\n    tokens = []\n    ner_tags = []\n    with open(filename, 'r', encoding='utf-8') as f:\n        words = []\n        tags = []\n        for line in f:\n            line = line.strip()\n            if not line:\n                if words:\n                    tokens.append(words)\n                    ner_tags.append(tags)\n                    words = []\n                    tags = []\n            else:\n                splits = line.split()\n                if len(splits) >= 2:\n                    words.append(splits[0])\n                    tags.append(splits[1])\n        if words:\n            tokens.append(words)\n            ner_tags.append(tags)\n    return {'tokens': tokens, 'ner_tags': ner_tags}\n\n# 加载您的数据\ntrain_data = read_conll_data('./data/train.txt')\nvalidation_data = read_conll_data('./data/validation.txt')\n\ntrain_dataset = Dataset.from_dict(train_data)\nvalidation_dataset = Dataset.from_dict(validation_data)\n\n# 创建数据集对象\ndatasets = DatasetDict({'train': train_dataset, 'validation': validation_dataset})\n```\n\n\n\n\n\n### 数据预处理\n\n```python\n# 定义数据预处理函数\ndef tokenize_and_align_labels(examples):\n    tokenized_inputs = tokenizer(\n        examples['tokens'],\n        truncation=True,\n        is_split_into_words=True,\n        padding='longest',  # 启用填充\n        max_length=512,     # 可根据需要调整\n    )\n    labels = []\n    for i in range(len(tokenized_inputs['input_ids'])):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        label_ids = []\n        previous_word_idx = None\n        for word_idx in word_ids:\n            if word_idx is None:\n                label_ids.append(-100)\n            elif word_idx != previous_word_idx:\n                label_ids.append(label_to_id.get(examples['ner_tags'][i][word_idx], label_to_id['O']))\n            else:\n                label_ids.append(-100)\n            previous_word_idx = word_idx\n        labels.append(label_ids)\n    tokenized_inputs['labels'] = labels\n    return tokenized_inputs\n# 预处理数据\ntokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)\n\n```\n\n\n\n### 评估指标\n\n```python\n\n# 加载评估指标\nmetric = evaluate.load('seqeval')\n\n# 定义评估函数\ndef compute_metrics(p):\n\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n\n    true_labels = [\n        [label_list[l] for l in label if l != -100]\n        for label in labels\n    ]\n    true_predictions = [\n        [label_list[pred] for (pred, lab) in zip(prediction, label) if lab != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n\n    results = metric.compute(predictions=true_predictions, references=true_labels)\n    return {\n        'precision': results['overall_precision'],\n        'recall': results['overall_recall'],\n        'f1': results['overall_f1'],\n        'accuracy': results['overall_accuracy']\n    }\n\n# 定义 DataCollator\ndata_collator = DataCollatorForTokenClassification(\n    tokenizer,\n    padding='longest',\n    max_length=512,\n    return_tensors='pt'\n)\n\n```\n\n\n\n### 定义trainer\n\n因为要做知识蒸馏，所以需要自定义trainer，结合教师模型的结果来做损失判定\n\n```python\n\n# 定义自定义 Trainer，用于知识蒸馏\nclass DistillationTrainer(Trainer):\n    def __init__(self, teacher_model, temperature=2.0, alpha=0.5, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.teacher_model = teacher_model\n        self.temperature = temperature\n        self.alpha = alpha\n        self.teacher_model.eval()\n\n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        labels = inputs.pop(\"labels\").to(device)\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        outputs = model(**inputs)\n        student_logits = outputs.logits\n\n        # 计算学生模型的损失（交叉熵损失）\n        loss_fct = nn.CrossEntropyLoss()\n        active_loss = labels.view(-1) != -100\n        active_logits = student_logits.view(-1, self.model.config.num_labels)[active_loss]\n        active_labels = labels.view(-1)[active_loss]\n        student_loss = loss_fct(active_logits, active_labels)\n\n        with torch.no_grad():\n            teacher_outputs = self.teacher_model(**inputs)\n            teacher_logits = teacher_outputs.logits\n\n        # 提取学生模型中对应于原始标签的 logits\n        num_labels_teacher = teacher_logits.size(-1)  # 7\n        student_logits_for_kd = student_logits[:, :, :num_labels_teacher]\n\n        # 计算蒸馏损失（KL 散度）\n        loss_fct = nn.KLDivLoss(reduction='batchmean')\n        student_logits_temp = student_logits_for_kd / self.temperature\n        teacher_logits_temp = teacher_logits / self.temperature\n\n        distillation_loss = loss_fct(\n            F.log_softmax(student_logits_temp, dim=-1),\n            F.softmax(teacher_logits_temp, dim=-1)\n        ) * (self.temperature ** 2)\n        print(\"Student logits shape:\", student_logits.shape)\n        print(\"Teacher logits shape:\", teacher_logits.shape)\n        print(\"Student logits for KD shape:\", student_logits_for_kd.shape)\n        # 合并损失\n        loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n\n        return (loss, outputs) if return_outputs else loss\n\n# 定义训练参数，使用较小的学习率\n# 定义训练参数\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    logging_dir='./logs',\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=3e-5,\n    dataloader_pin_memory=False,\n)\n\n# 初始化自定义 Trainer\ntrainer = DistillationTrainer(\n    teacher_model=teacher_model,\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets['train'],\n    eval_dataset=tokenized_datasets['validation'],\n    compute_metrics=compute_metrics,\n    data_collator=data_collator,\n)\n\n# 开始训练\ntrainer.train()\n```\n\n\n\n\n\n### 最后保存模型\n\n``` python\n# 保存模型和分词器\ntrainer.save_model('./my_trained_model')\ntokenizer.save_pretrained('./my_trained_model')\n```\n\n\n\n\n\n### 结论\n\n目前测试下来，如果整体数据集都是全新的数据，增加教师模型对于模型训练帮助不大，不如直接开始微调基座模型，因为标签和数据都是未曾见过的，教师模型无法给出建议，只会有干扰，但是这对于新数据和旧数据有一些重叠的情况会有所帮助。\n","tags":["deep learning","python","ner"],"categories":["技术文章"]}]
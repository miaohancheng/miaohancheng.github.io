[{"title":"文本大模型rag检索","url":"/2025/04/28/文本大模型rag检索/","content":"\n\n\n### 背景\n\n在工作中会遇到有些文本的描写、语序、语种出现变幻，但是大体上意思是可以匹配上一些已知知识库的时候，原本的文本embedding向量检索是可以解决问题的，但是当语言含义需要结合现实知识理解后才能匹配，就需要用大模型先行理解，在做匹配\n\n\n\n### 模型选取\n\n目前是选用了 阿里开源的[qwq](https://huggingface.co/Qwen/QwQ-32B)，使用ollama快速部署\n\n```shell\nollama run qwq\n```\n\n也对比过其他模型，比如：[DeepSeek-R1-Distill-Qwen-32B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)、[DeepSeek-R1-Distill-Llama-8B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)、[Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B)，还有几个别的模型，目前测下来还是qwq的效果最好\n\n\n\n### 代码结构\n\n1. 加载内部知识库，同时进行处理，处理完成后的内容通过 [nomic-embed-text](https://huggingface.co/nomic-ai/nomic-embed-text-v1.5)来进行embedding，完成后导入本地FAISS，作为先验知识\n2. 大模型先理解，解析数据后进行匹配，如果匹配上则直接返回输出\n3. 如果步骤2匹配不到，大模型进行理解补全，规范化之后再进行匹配，匹配上则直接返回\n\n为什么要加上3，因为现实生活中很多场景下实际上的输入是简称或者缩写，人类可以理解但是直接向量匹配有难度，需要大模型理解补全后进行匹配\n\n\n\n### 代码逻辑\n\n引入langchain、模型服务由ollama提供\n\n```python\nimport os\nimport pandas as pd\nfrom langchain_ollama import OllamaEmbeddings, OllamaLLM  \nfrom langchain_community.vectorstores import FAISS\nfrom langchain.schema import Document\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\nimport traceback\n\n```\n\n\n\n配置模型后端服务，并测试\n\n```python\n# -----------------------------\n# 配置 LLM（用于后处理重排序以及地址补全）\nllm = OllamaLLM(\n    model=\"qwq\",   \n    temperature=0.3,\n    max_tokens=4096\n)\ntry:\n    response = llm.invoke('你好')\n    print(\"LLM 返回：\", response)\nexcept Exception as e:\n    print(\"LLM 调用失败，请检查模型配置：\", e)\n\n```\n\n\n\n加载先验知识库（这里以一个地址信息为例，开源数据地址：[US-Cities-Database](https://github.com/kelvins/US-Cities-Database)）\n\n```python\n# -----------------------------\n# 加载区划配置数据\ncolumn_names = ['REGION', 'COUNTY', \n             'CITY',  'LONGITUDE', 'LATITUDE','POST_CODE_LIST']\ndf = pd.read_csv(\"data.txt\", sep=\"|\", header=None, names=column_names)\nprint(\"原始配置数据预览：\\n\", df.head())\n```\n\n| CITY     | COUNTY         | LATITUDE  | LONGITUDE  |\n| -------- | -------------- | --------- | ---------- |\n| Adak     | Aleutians West | 55.999722 | -161.20777 |\n| Akiachak | Bethel         | 60.891854 | -161.39233 |\n| Akiak    | Bethel         | 60.890632 | -161.19932 |\n| Akutan   | Aleutians East | 54.143012 | -165.78536 |\n| Alakanuk | Kusilvak       | 62.746967 | -164.60228 |\n\n\n\n构建文档，把pd转换成可用于检索的docs\n\n```python\n\n# -----------------------------\n# 构建候选 Document 集合（针对所有数据，后续检索时再做过滤）\ndef build_candidate_documents(df_all):\n    candidate_docs = []\n    for _, row in df_all.iterrows():\n        text = (\n            f\"国家: {row['REGION']} | 州: {row['COUNTY']} | \"\n            f\"城市: {row['CITY']} | \"\n            f\"邮编: {row['POST_CODE_LIST']}\"\n        )\n        metadata = {\n            \"REGION\": row['REGION'],\n            \"POST_CODE_LIST\": row['POST_CODE_LIST']\n        }\n        candidate_docs.append(Document(page_content=text, metadata=metadata))\n    return candidate_\n```\n\n\n\n初始化embedding模型\n\n```python\n\n\n# -----------------------------\n# 初始化嵌入模型\ntry:\n    embeddings = OllamaEmbeddings(\n        model='nomic-embed-text', \n        base_url=\"http://localhost:11434\",\n        temperature=0.3\n    )\nexcept Exception as e:\n    print(\"初始化 Embeddings 失败，请检查模型配置：\", e)\n    raise\n\n```\n\n\n\nFAISS索引（为了加快速度，我做了一个缓存，实际上可以每次都重新生成）\n\n```python\n\n\n# -----------------------------\n# FAISS 索引缓存设置\nINDEX_DIR = \"faiss_index_dir\"\nif os.path.exists(INDEX_DIR):\n    faiss_index = FAISS.load_local(INDEX_DIR, embeddings, allow_dangerous_deserialization=True)\n    print(\"加载了本地 FAISS 索引\")\nelse:\n    candidate_docs = build_candidate_documents(df)\n    faiss_index = FAISS.from_documents(candidate_docs, embeddings)\n    faiss_index.save_local(INDEX_DIR)\n    print(\"构建并缓存了 FAISS 索引。\")\n```\n\n\n\n定义好提示词模板和要求的结果格式：\n\n```python\n\n# -----------------------------\n# 定义后处理重排序的提示模板\nrerank_prompt_template = \"\"\"\n以下为候选区划匹配结果：\n{context}\n\n用户输入的地址为：{input}\n\n请根据候选结果及用户输入，选择最匹配的一条（忽视大小写），并严格按照以下格式返回结果：\nREGION: [国家]\nCOUNTY: [州]\nCITY: [城市]\n请仅返回匹配结果，不添加其他解释性文字。\n\"\"\"\nrerank_prompt = PromptTemplate(\n    template=rerank_prompt_template,\n    input_variables=[\"context\", \"input\"]\n)\ndocument_chain = create_stuff_documents_chain(llm, rerank_prompt)\n\n```\n\n\n\n定义如果第一次匹配不到，补全的提示词和返回结果\n\n```python\n\n# -----------------------------\n# 定义大模型补全地址的提示模板和函数\ncomplete_address_template = \"\"\"\n用户输入的地址为：{input}\n已知国家为：{region}\n用户输入的有可能是快递中转点、机场、城市名、城市简写等，请你综合所有的知识来判断，补全地址的语种与用户输入的语种一致,州不需要加 state county 等描述\n请将该地址补全为标准格式的城市，州，国家，格式如下：\nREGION: [国家]\nCOUNTY: [州]\nCITY: [城市]\n\n请仅返回补全后的地址，不附加其他解释性文字。\n\"\"\"\ncomplete_address_prompt = PromptTemplate(\n    template=complete_address_template,\n    input_variables=[\"input\", \"region\"]\n)\n```\n\n\n\n解析结果\n\n```python\nimport re\n\ndef parse_think_content(text):\n    \"\"\"解析包含think标签的文本并提取结构化地址\"\"\"\n    # 提取think块中的answer内容（支持多段分析场景）\n    answer_blocks = re.findall(r'</think>.*', text, re.DOTALL)\n    if not answer_blocks:\n        return None\n\n    # 初始化地址字典\n    address = {\n        '城市': '',\n        '州': '',\n        '国家': ''\n    }\n\n    # 从最后一个answer块中提取结构化地址（优先取最终结论）\n    last_answer = answer_blocks[-1].strip()\n    for line in last_answer.split('\\n'):\n        if '城市' in line:\n            address['城市'] = line.split(':')[-1].strip()\n        elif '州' in line:\n            address['州'] = line.split(':')[-1].strip()\n        elif '国家' in line:\n            address['国家'] = line.split(':')[-1].strip()\n\n    # 拼接为指定格式\n    return f\"{address['城市']},{address['州']},{address['国家']}\"\n\n\n```\n\n\n\n\n\n调用大模型补全\n\n```python\n\n\n\ndef complete_address(address_input, region):\n    prompt = complete_address_prompt.format(input=address_input, region=region)\n    try:\n        result = llm.invoke(prompt)\n\n        return result.strip()\n    except Exception as e:\n        print(\"调用大模型补全地址时失败：\", e)\n        return address_input\n\n```\n\n检查doc对象\n\n```python\n\n# -----------------------------\n# 辅助函数：确保候选项为 Document 对象\ndef ensure_document(doc):\n    if isinstance(doc, Document):\n        return doc\n    else:\n        return Document(page_content=str(doc), metadata={})\n```\n\n检索过滤\n\n```python\n\n# 检索后预过滤函数：基于用户输入的邮编和 region 过滤候选结果\ndef post_filter_candidates(user_input, user_region, candidates):\n    filtered = candidates\n    print(candidates)\n    if user_region:\n        filtered = [doc for doc in filtered if  doc.metadata.get(\"region\", \"\").lower() == user_region.lower()]\n    print(f\"基于 region {user_region} 过滤后候选数：{len(filtered)}\")\n    postal_pattern = re.compile(r\"[\\s,;+]*([0-9]{5}(?:-[0-9]{4})?)\")\n    postal_match = postal_pattern.search(user_input)\n    if postal_match:\n        postal_code = postal_match.group(1)\n        filtered = [doc for doc in filtered if postal_code.lower() in doc.metadata.get(\"post_code_list\", \"\").lower()]\n        print(f\"用户输入中检测到邮编 {postal_code}，过滤后候选数：{len(filtered)}\")\n\n    return filtered\n\n```\n\n\n\n主要逻辑\n\n```python\n\n# -----------------------------\n# 主交互模块\nif __name__ == \"__main__\":\n    print(\"=\" * 40)\n    print(\"智能地址区划匹配系统（全量向量化缓存+检索后预过滤+后处理重排序）\")\n    print(\"输入 'exit' 退出程序\")\n    print(\"=\" * 40)\n\n    retriever = faiss_index.as_retriever(\n        search_type=\"similarity\",\n        search_kwargs={\"k\": 20, \"score_threshold\": 0.7},\n        return_source_documents=True\n    )\n\n    while True:\n        user_input = input(\"\\n请输入待匹配的地址文本：\").strip()\n        user_input = user_input.replace('+', ',')\n        if user_input.lower() == 'exit':\n            break\n        user_region = input(\"请输入对应的 region：\").strip()\n\n        print(\"正在定位...\")\n\n        try:\n            # 第一次检索与过滤\n            retrieved_docs = retriever.invoke(user_input)\n            filtered_docs = post_filter_candidates(user_input, user_region, retrieved_docs)\n\n            # 若没有匹配到任何候选结果，则尝试调用大模型补全地址，并重新检索\n            if not filtered_docs:\n                print(\"未匹配到任何数据，尝试通过大模型补全地址...\")\n                new_input = complete_address(user_input, user_region)\n                print(\"大模型补全地址思考: \", new_input)\n                new_input = parse_think_content(new_input)\n                print(\"大模型补全后的地址：\", new_input)\n                retrieved_docs = retriever.invoke(new_input)\n                filtered_docs = post_filter_candidates(new_input, user_region, retrieved_docs)\n                if not filtered_docs:\n                    print(\"补全地址后仍未匹配到任何区划数据，放弃该地址。\")\n                    continue\n                else:\n                    user_input = new_input\n\n            # 根据候选数量决定是否调用 LLM 后处理重排序\n            if len(filtered_docs) > 1:\n                rerank_result = document_chain.invoke({\n                    \"context\": filtered_docs,\n                    \"input\": user_input\n                })\n                print(\"\\n匹配结果：\")\n                print(rerank_result)\n            else:\n                print(\"\\n匹配结果：\")\n                single_doc = filtered_docs[0]\n                print(single_doc.page_content if hasattr(single_doc, \"page_content\") else str(single_doc))\n        except Exception as e:\n            traceback.print_exc()\n            print(f\"处理失败：{str(e)}\")\n```\n\n\n\n\n\n### 运行效果\n\n1. 直接就能匹配到：\n\n```json\n========================================\n智能地址区划匹配系统（全量向量化缓存+检索后预过滤+后处理重排序）\n输入 'exit' 退出程序\n========================================\n\n请输入待匹配的地址文本：33170,fl\n请输入对应的 region：united states\n正在定位...\n基于 region united states 过滤后候选数：20\n用户输入中检测到邮编 33170，过滤后候选数：1\n\n匹配结果：\n国家: United States | 州: Florida | 城市: Goulds \n```\n\n\n\n\n\n\n\n\n\n2. 需要补全才能匹配上\n\n```json\n========================================\n智能地址区划匹配系统（全量向量化缓存+检索后预过滤+后处理重排序）\n输入 'exit' 退出程序\n========================================\n\n请输入待匹配的地址文本：lax\n请输入对应的 region：United States\n正在定位...\n[]\n基于 region United States 过滤后候选数：0\n未匹配到任何区划数据，尝试通过大模型补全地址...\n大模型补全地址思考:  <think>\n好的，我现在需要处理用户输入的地址“lax”，并将其补全为城市、州和国家的标准格式。首先，我得确定用户可能指的是什么。\n\n用户提到输入可能是快递中转点、机场、城市名或简写等。LAX 是一个常见的缩写，最著名的应该是洛杉矶国际机场（Los Angeles International Airport）。不过也有可能指其他地方，但通常LAX主要关联的是这个机场。接下来需要确认所属的行政区划。\n\n洛杉矶国际机场位于美国加利福尼亚州洛杉矶市。因此，城市通常是具体的市或区。这里应该填洛杉矶市（City of Los Angeles）。州是加州（California），一级地址自然是United States。用户要求州不需要添加state、county等描述，所以直接写California即可。\n\n需要确保所有名称都是完整的正式名称，并且语种一致，用户输入的是英文缩写，所以补全后的地址也应使用英文。检查是否有其他可能性，比如是否存在以LAX命名的其他城市或地区，但考虑到国际通用性，LAX作为机场代码更为常见，因此确定上述结构是正确的。\n\n</think>\n\n城市: Los Angeles\n州: California\n国家: United States\n大模型补全后的地址： Los Angeles,California,United States\n\n基于 lv1_name United States 过滤后候选数：20\n\n匹配结果：\n<think>\n好的，我需要根据用户提供的地址“Los Angeles, California, United States”从候选区划中找到最匹配的一条。首先，用户输入的是国家United States，州是California，城市是Los Angeles。不过候选中的城市有多个包含“Los Angeles”的选项，比如Lake Los Angeles、Los Angeles Afb、West Los Angeles、East Los Angeles等。\n\n我需要仔细看一下每个候选条目。用户直接写的是Los Angeles，没有更具体的区域，所以可能要找最接近的匹配。注意到有一个城市是“Los Angeles Afb”，邮编90009。但可能存在更准确的选项吗？\n\n不过候选列表中并没有一个完全精确匹配“Los Angeles”作为城市的情况，因为通常洛杉矶市（City of Los Angeles）可能没有被单独列为一个条目，而是分为不同的区域如West、East等。这时候需要判断哪个最接近用户输入。\n\n或者可能用户输入的地址是通用的，而候选中的选项更具体。例如，“Los Angeles Afb”可能指的是洛杉矶空军基地，但用户可能指整个城市。不过根据提供的候选列表中没有完全匹配“Los Angeles”的城市名称，所以必须选择最接近的。\n\n比较各个选项，其中“Los Angeles Afb”虽然带有Afb（可能是空军基地），但城市名称中最接近用户输入的是它，因为其他如Lake Los Angeles和East/West Los Angeles都是更具体的区域。因此可能选Los Angeles Afb作为匹配？\n\n或者是否有可能我遗漏了某个条目？再仔细检查一遍候选列表：\n\n候选中的城市有：\n- Lake Los Angeles\n- Los Angeles Afb\n- West Los Angeles\n- East Los Angeles\n- Hollywood（属于LA市的一部分）\n- Beverly Hills（虽然属于洛杉矶县，但州是CA）\n\n用户输入的是Los Angeles作为城市，而可能正确的应该是整个城市名称。但候选中没有直接的“Los Angeles”条目，所以需要选最接近的。可能用户指的是整个城市，但候选中的选项更细分了区域。这时候可能需要选择邮编覆盖市中心或主要区域的那个？\n\n例如，“Los Angeles Afb”的邮编是90009，而洛杉矶市中心的一些邮编可能属于其他区域。或者West Los Angeles的邮编包括90028等。不过用户输入的是最通用的形式，没有具体到东西湖或空军基地，所以可能需要选择最接近名称匹配的选项，即“Los Angeles Afb”？\n\n但另一个可能性是，虽然名称不完全相同，但用户可能指的是整个洛杉矶市，而候选中可能将该市分为不同区域。因此，在这种情况下，可能应该选择邮编覆盖主要区域的那个条目？或者根据名称相似度，最接近的是“Los Angeles Afb”，因为其他选项如Lake Los Angeles明显是更小的社区。\n\n不过也有可能用户输入的地址中的城市其实是洛杉矶县（County），但候选中的州已经是California，所以城市应该是城市或地区。可能在这种情况下，“Los Angeles Afb”是最直接匹配名称的部分，因此选它？\n\n或者是否还有其他考虑因素？比如邮编是否覆盖更广泛的区域？例如，如果用户没有提供具体地址部分，那么最合适的可能是最通用的选项。但候选中并没有这样的条目。\n\n综上所述，在提供的候选列表中最接近用户输入“Los Angeles”的城市名称是“Los Angeles Afb”，因此选择该条目。\n</think>\n\n国家: United States\n州: California\n城市: Los Angeles Afb\n```\n\n\n\n3. 从候选中思考后选择最匹配的（无需补全）\n\n```json\n========================================\n智能地址区划匹配系统（全量向量化缓存+检索后预过滤+后处理重排序）\n输入 'exit' 退出程序\n========================================\n\n请输入待匹配的地址文本：north miami beach ,fl\n请输入对应的 region：united states\n正在定位...\n基于 region united states 过滤后候选数：7\n\n匹配结果：\n<think>\n好的，我需要处理用户的查询，他们输入的地址是“north miami beach, fl”，并从给定的候选数据中找到最匹配的一条。首先，我要仔细分析用户提供的每个候选信息。\n\n首先，用户输入中的关键词是“North Miami Beach”和“FL”。根据问题要求，要忽视大小写，所以需要将所有选项转换为统一的大小写形式进行比较。接下来，我会逐一查看各个候选区划：\n\n1. **North Miami Beach**：城市名称正好与用户的输入完全匹配，并且州简称是FL，符合用户提供的州缩写。其邮编包括33181、33179等，可能覆盖用户提到的地区。\n\n2. **North Miami**：虽然名字中有“North Miami”，但缺少“Beach”部分，与用户的输入不完全一致。不过需要确认是否存在拼写错误或简称的情况，但根据问题要求应严格匹配名称。\n\n3. 其他选项如North Palm Beach、North Redington Beach、Miami Beach等，名称差异较大，明显不符合用户输入的“North Miami Beach”。\n\n\n此外，用户的输入中有逗号分隔城市和州，通常格式为“City, State”。这里用户明确写了FL（佛罗里达州），与候选中的州一致。\n\n综上所述，最匹配的应该是第一个选项：North Miami Beach。\n</think>\n\n国家: United States\n州: Florida\n城市: North Miami Beach\n\n```\n\n\n\n理解简写后匹配\n\n```json\n========================================\n智能地址区划匹配系统（全量向量化缓存+检索后预过滤+后处理重排序）\n输入 'exit' 退出程序\n========================================\n\n请输入待匹配的地址文本：jfk,ny,us\n请输入对应的 region：united states\n正在定位...\n基于 lv1_name united states 过滤后候选数：20\n\n匹配结果：\n<think>\n嗯，用户输入的地址是“jfk, ny, us”。我需要从候选区划中找到最匹配的一条。首先，国家应该是United States，因为用户用了US，而所有候选项都是国家为United States。\n\n接下来州是NY，对应New York州。现在要看城市部分。用户输入的JFK应该是指约翰·菲茨杰拉德·肯尼迪国际机场，通常缩写为JFK。在候选列表里有几个可能的选项：\n\n第一个候选条目就是John F Kennedy Airpor，邮编11430。另一个是Kennedy，邮编14747。这两个名字都和JFK相关，但哪一个更准确呢？\n\n通常来说，纽约的肯尼迪机场正式名称是John F. Kennedy International Airport，所以全称应该是John F Kennedy Airport。而另一个Kennedy可能是指其他地方，比如纽约州内的某个镇或区域？不过邮编14747查一下是否属于该机场附近？或者可能Kennedy是简称？\n\n用户输入的是jfk，这通常直接对应到John F Kennedy Airport这个全称。所以应该选第一个条目。\n\n其他选项比如Roosevelt Island、Loehmanns Plaza这些显然不符合JFK的缩写。而Florida的那个Kennedy Space Center虽然也有Kennedy，但州是Florida，用户明确用了NY，所以排除。\n\n因此最匹配的是第一个条目。\n</think>\n\n国家: United States\n州: New York\n城市: John F Kennedy Airport\n```\n\n","tags":["deep learning","python","llm","rag"],"categories":["技术文章"]},{"title":"巴菲特芒格投资笔记","url":"/2025/03/10/巴菲特芒格投资笔记/","content":"\n\n\n\n**核心投资哲学**\n\n​\t•\t**价值投资原则**：巴菲特和芒格坚持以企业的内在价值为基础进行投资，强调“价格是你所付出的，价值是你所得到的”。他们把股票看作所投资企业的一部分，而非仅仅是交易代码，注重企业长期的盈利能力和增长潜力，而非市场情绪的短期波动  。巴菲特引用导师本杰明·格雷厄姆的比喻描述理想的投资心态：把市场先生视为服务于你的伙伴而非指导你的老师——当市场情绪极度乐观或悲观时，明智的投资者要么 **忽略** 市场先生的报价，要么 **利用** 它带来的机会 。正因如此，他们强调在买入时留有足够的“安全边际”，以便即使估值判断稍有错误也不会招致永久性损失（芒格亦告诫“不应轻易打断复利的进程”，即避免因为短期波动而卖出优质资产）。总的来说，他们奉行 **“第一条规则是不亏损，第二条是记住第一条”** 的原则，以确保长期资本稳健增值。\n\n​\t•\t**长期主义与复利效应**：巴菲特和芒格深知长期持有优秀企业所带来的复利威力。他们常引用爱因斯坦的话将复利比作“世界第八大奇迹”，并以自身实践证明其威力：伯克希尔从几十年前的小型合伙企业成长为市值逾千亿美元的投资巨头，即是长期复利积累的结果。他们主张以**多年甚至数十年**为尺度来看待投资，“时间是优秀企业的朋友，却是平庸企业的敌人” 。也就是说，如果买入的是一家“了不起的公司”，时间拉长后价值终将体现；反之，平庸公司纵使暂时便宜，长期持有往往问题不断。这体现了长期主义的精髓：通过 **耐心持有** 优质资产，让复利为你工作。芒格甚至形象地指出：“不要因为一时心急而拔苗助长，也不要在复利增长过程中轻易中断。”正是这种耐心和纪律，使他们能够跨越牛熊周期获取超额收益。\n\n​\t•\t**投资与投机的区别**：在巴菲特和芒格看来，投资与投机有本质区别。**投资** 是基于企业基本面价值的判断，期待从企业长期经营成果中获利；**投机** 则是赌市场情绪和价格波动，意在从下一笔交易中套利。巴菲特形象地说，投资者关注的是企业将来的经营状况，投机者关注的是股票近期的价格走势。他们强调投资应有 **主人翁心态** ——购买股票就意味着购入了一家企业，应当像经营企业那样思考。而投机者往往**情绪化追涨杀跌**，容易受到“贪婪与恐惧”这两种传染病的支配。对此巴菲特的忠告是：“在别人贪婪时恐惧，在别人恐惧时贪婪” 。也就是说，当市场疯狂炒作时要保持冷静谨慎；当市场恐慌抛售优质资产时反而要敢于贪婪地逢低吸纳。这种逆向思维正是基于对投资与投机区别的清醒认识：真正的投资是在市场情绪最悲观时播下种子，在漫长岁月中等待价值开花结果。\n\n\n\n**选股标准**\n\n​\t•\t**长期竞争优势（护城河）**：巴菲特提出伟大公司的首要特征是拥有 **持久的经济护城河**。也就是企业具有难以被竞争对手复制或超越的竞争优势，能够长期获得超额回报 。这种护城河可以来源于强大的品牌（例如可口可乐在全球拥有独一无二的品牌认知度）、网络效应、专利技术、成本领先（如GEICO通过低成本运营获得优势）等。不论形式如何，护城河能保护企业的“城堡”不被竞争对手攻破，使其在岁月流逝中依然保持高额的投资回报率 。反之，没有护城河的企业其高盈利很快会吸引竞争，利润将被侵蚀。正如巴菲特所警示的：“资本主义的运作保证了竞争者会反复冲击任何高利润的‘城堡’。因此，一个**强大且持久的护城河**（例如极低的成本或全球知名品牌）对持续成功至关重要。商业史上有太多‘罗马烛火’式的公司，它们的护城河被证明只是幻觉，转瞬即逝” 。因此，巴菲特和芒格在选股时格外关注企业是否具备 **可延续几十年的独特优势**，这也是他们钟爱可口可乐、喜诗糖果（See’s Candies）等公司的原因所在。\n\n​\t•\t**优秀的管理层和企业文化**：管理层的诚信与能力是巴菲特和芒格选股时的另一大考量。他们强调只与值得信赖、聪明能干的管理者“同舟共济”。巴菲特坦言：“我们寻找由能干而诚实的管理者经营的公司” 。在他们看来，卓越的管理层能合理分配资本、稳健经营，并以股东利益为重。芒格也一再强调**诚信**的重要性，认为没有诚信的聪明反而会害了公司。他们喜欢投资那些管理层以身作则、注重长期价值而非短期收益的企业，并且企业文化健康向上。例如，巴菲特称赞苹果公司的CEO蒂姆·库克是“最出色的管理者之一”，对业务了如指掌且管理公司井井有条 ；这样的“诚实贤能的掌舵人”让他们对企业充满信心。相反，即使企业本身有潜力，如果管理层水平低下或道德有亏，他们也避而远之。可以说，**选人** 与 **选企业** 同样重要，正如巴菲特所言：“好骑师难救劣马”，管理再优秀，若产业本质不好也无回天之力 ；但反过来讲，再好的赛马（好生意）也需要可靠的骑师（好管理）来发挥潜能。\n\n​\t•\t**财务稳健性**：巴菲特和芒格偏好财务结构稳健、盈利能力强的公司。具体来说，他们寻找那些**利润率高、股本回报率(ROE)出色且负债水平低**的企业。这类公司通常意味着经营效率高、内生现金流充沛，不需要过度依赖外部债务融资。巴菲特在2019年致股东信中总结理想投资标的的标准首先就是：“它能够在其运营所需的有形资本上获得良好的回报” 。持续高ROE通常是公司拥有护城河的体现。同时，他也警惕高杠杆风险，倾向于企业保持稳健的资产负债表。在选股时，他们会仔细研读财务报表，关注营业利润率、自由现金流等指标，以确认公司**盈利模式健康可持续**。例如，伯克希尔长期持有的喜诗糖果每年都能产生可观的现金盈余且几乎无债务，这正是财务稳健带来持续价值创造的实例。相反，财务数据动荡、债台高筑的公司往往被他们排除在投资名单之外。\n\n\n\n此外，巴菲特和芒格通常只投资他们**懂的业务**。所谓“能力圈”概念，即在自己理解的行业领域内选股。他们回避那些看不懂或快速变化的行业，因为无法判断其中公司的长期经济特性。这个原则贯穿于选股标准的方方面面：只有在理解业务模型的前提下，才能评估护城河是否稳固、管理层决策是否明智、财务数字是否可信。因此，他们的投资组合中多是消费品、金融、工业等相对熟悉的领域，而对生物科技、新兴互联网等超出能力圈的机会则宁可错过也不贸然涉足。这种自律使他们避免了很多看似热门实则风险难测的投资，保证了投资标准的一贯性和可靠性。\n\n\n\n**买入决策**\n\n​\t•\t**估值方法（合理价格买入）**：巴菲特常说：“以合理的价格买入一家出色的公司，远胜于以出色的价格买入一家平庸的公司” 。这反映了他在估值上的核心理念：**宁可为高品质企业支付略高于账面估值的合理价，也不要因为便宜而买入一家前景黯淡的平庸公司**。具体而言，他们会先计算出企业的内在价值（基于未来现金流折现或资产价值等），然后只在股价显著低于内在价值时才买入，从而留出“安全边际”。巴菲特将这种低估买入的机会比喻为捡便宜的雪茄烟蒂（只剩一口却非常便宜）和购买璀璨的希望钻石（一部分胜过一堆廉价仿制品）之间的权衡 。早年他曾热衷于“廉价捡漏”式投资，但后来在芒格影响下转向“以公道价格买入优质企业”的策略，这一点从他收购喜诗糖果的经历中得到印证。估值时他们也关注**绝对收益率**：预期买入后能获得的股东收益率（包括盈利增长和股息）必须显著高于无风险利率和市场平均水平，才值得出手。总之，他们的买入决策建立在严谨的估值分析基础上，既不会盲目追高（坚决回避“高估值炒作”），也不会因为价格便宜就忽视质量。只有当**“价格<价值”**且有足够余地时，他们才会果断买入。\n\n​\t•\t**何时买入（逆向思维）**：在择时方面，巴菲特和芒格秉持逆向投资的思维——**在他人恐惧时贪婪**。当市场出现非理性恐慌、优质公司股价大跌时，他们往往视之为千载难逢的买入良机。例如1973-74年熊市中巴菲特大举买入华盛顿邮报，公司基本面并未受损但股价跌去大半，结果获取巨额回报。对此巴菲特在致股东信中直言，他们从不试图预测市场何时见顶或见底，也无法预知贪婪或恐惧何时蔓延，但可以确定的是市场周期性的情绪**终将带来价格大幅偏离价值的机会**。他说：“我们只是力求在别人贪婪时保持恐惧，而在别人恐惧时变得贪婪” 。因此，当优质股票因短期利空被恐慌抛售、股价远低于内在价值时，他们会大胆地逢低买入。而当市场狂热、股票被疯狂追捧导致估值远超内在价值时，他们会格外谨慎，不会跟风投入。巴菲特形容自己“别人的宏观预测听归听，但从不据此调整投资”，更多是**静待机会**而非主动择时。实际上，许多伯克希尔的重大投资都是在市场低迷时期完成的：例如1987年股灾后的可口可乐、2008年金融危机中的高盛优先股等。可见，他们的买入时机往往逆大众情绪而动，在市场最悲观时坚定地为未来布局。\n\n​\t•\t**分批与集中**：在具体执行上，巴菲特倾向于在估值合理或低估时**集中买入**足够多的仓位，而非每天小额频繁交易。他曾打趣“好机会并不常有，一旦碰上就要**用力挥棒**（全力出击）”。这意味着如果判断某只股票非常有吸引力，他会果断地买入大量股票建立重仓，而不像一般投资者那样畏首畏尾、小打小闹。同时，他们也会分批建仓以优化成本：比如分几次在下跌过程中逐步买入，从而摊低整体成本价。在买入决策上，芒格比喻为“捡起地板上的准一美元硬币，用五毛钱的价格买下”，这样的好买卖出现时要集中火力。而当手头没有满意的投资标的时，他们宁可让现金闲置，也不会为了“资金利用率”去随便买入次优资产。正因为这种耐心等待和集中出击的策略，伯克希尔的投资往往**命中率极高**，很少出现频繁调仓的情况。\n\n\n\n**持有策略**\n\n​\t•\t**长期持有**：巴菲特名言：“我们最喜欢的持有期是永远” 。对于符合他们标准的优秀公司，一旦买入，他们往往持股多年乃至数十年之久，把时间当作朋友，充分享受企业成长带来的复利效应。他们与那些稍有涨幅就急于获利了结、或遇到风浪就惊慌抛售的投资者截然不同，而是更像耐心的园丁，种下好树后静待其成长茁壮。例如，可口可乐公司自1988年买入后，伯克希尔已持有超过30年，从未因为短期波动而卖出，其市值和分红回报已令初始投资增长了几十倍。同样，喜诗糖果自1972年收购后，每年贡献的现金流源源不断地为伯克希尔创造价值。芒格曾幽默地说，**“坐着不动”** 是投资中最容易被低估的本领：找到值得长期持有的好股票后，最明智的做法往往是什么也不做，让复利自行滚雪球。因此，除非有非常特殊的情况（见下条），他们一般不会轻易卖出优质持仓。\n\n​\t•\t**何时卖出**：尽管倾向永久持有，巴菲特和芒格也承认有少数情形会考虑卖出股票。首先是 **基本面恶化**：如果一家公司的经济护城河缩小甚至消失，或者管理层发生不可接受的改变，使得当初买入的投资逻辑不再成立，他们会果断地退出投资。巴菲特表示，他们买股票并没有预设卖出的时间或价格，**只要企业的内在价值能以令人满意的速度增长，就会一直持有** ；反之，如果发现企业前景不妙，增长停滞甚至倒退，则不恋战，及时止损。其次是 **更好的机会出现**：如果手头某持仓相对于其内在价值已充分反映（甚至高估），而市场上出现了一个更具吸引力的新投资机会，他们可能会调仓。巴菲特将这种再平衡看作资本再配置的正常操作，但他也提醒**不要过度频繁换马**，因为优秀机会毕竟少数，大部分情况下“手中鸟”胜过“林中两鸟”。再次，有时出于 **投资组合考虑或政策因素** 也会部分减持，例如伯克希尔曾在持股比例过高或监管限制时略微减持某些股票（如苹果在2020年被动减持一些以避免持股比例过大）。总的来说，他们卖出的频率远低于买入，一旦选定公司，默认策略是伴随公司成长一路同行，除非上述条件迫使他们分手。\n\n​\t•\t**应对市场波动**：巴菲特和芒格对市场波动的看法是 **“不预测、不恐惧、善加利用”**。他们不会因为股价每日波动而改变对企业价值的判断，也不尝试抓住每一次涨跌赚快钱。相反，他们把市场视为情绪多变的伙伴“市场先生”(Mr. Market) 。市场先生每日报出价格，但有时过于乐观、有时过度悲观，报价常常偏离实际价值。然而，**市场先生乐于被忽略**：如果当天价格不合意，可以什么也不做；如果价格荒谬地低，则是捡便宜的好机会 。这种心态使他们在股市大幅下挫时依然镇定，从基本面出发评估是否逢低加仓，而不是被恐慌情绪裹挟随大流卖出。同样，在股价非理性高涨时，他们也不会兴奋追高，而是警觉地审视风险。巴菲特打过比方，说即便证券市场关闭几年，对他们影响也不大，因为**企业的长期业绩终将决定投资成败**，短期行情只是提供报价的噪音 。因此，他们应对波动的策略是：**平常心看待**，不被情绪牵着走；同时当波动导致价格大跌远低于价值时，**果断出手** 逆势买入优质资产。在这一进一退中，将市场波动由敌人变成了朋友。\n\n​\t•\t**加仓与减仓策略**：在持有期间，巴菲特和芒格也会根据情况调整仓位配置。对于前景依然看好且出现价格下跌的持仓，他们往往选择 **加仓**。只要确定公司基本面未变，股价下跌反而提供了更便宜的买入机会，巴菲特称这是“给喜爱的公司打折”——例如他多次在市场低迷时增持富国银行和美国运通等股票。当2008年金融危机重挫股市时，伯克希尔不仅没有抛售核心持股，反而在债券和优先股等领域大举投资，为日后收益打下基础。相反，对于那些情况变差的投资，他们会 **止损减仓** 或清仓，不恋战、不摊大亏。芒格形容这一进一出就像做减法：**“除去糟糕的，留住优秀的”**。值得注意的是，他们极少因为短期股价涨高就减仓锁定利润，如果公司依旧优秀，他们更倾向于一路拿牢，让利润奔跑。只有当某持仓的估值明显过高、未来回报预期很低时，才可能逐步减仓转向别处。同样地，如果持仓比例过大增加了风险（比如某一行业或单一股票权重过高），他们也可能适度调整以保持组合平衡。但总体而言，加减仓的频率都不高——正如巴菲特所言，他们更喜欢 **“懒惰”** 一点，耐心守着那些经过深思熟虑选出的赢家，而不是频繁买进卖出。\n\n\n\n**案例分析**\n\n\n\n在漫长的投资生涯中，巴菲特和芒格通过伯克希尔·哈撒韦进行了一系列经典投资，这些案例生动展现了上述投资理念的实践成果。以下选取成功和失败的代表性案例加以分析：\n\n​\t•\t**成功案例：可口可乐（Coca-Cola）** – 巴菲特于1988年在市场低迷之际大举买入可口可乐，当时耗资逾10亿美元获得公司约7%的股份 。他之所以钟情可口可乐，正是看中了其强大的品牌护城河和全球化潜力。当时可口可乐拥有全世界最具辨识度的品牌之一，产品“无可替代”，消费者忠诚度极高。这符合他对伟大企业的定义：消费者“需要或想要”它的产品，且“认为没有接近的替代品” 。可口可乐的商业模式简单清晰（浓缩液销售），毛利率极高，且在新兴市场有广阔增长空间。更重要的是，公司拥有稳健的管理层，当时CEO戈伊苏埃塔(Roberto Goizueta)大刀阔斧拓展全球业务，提升股东回报。巴菲特预见到，可口可乐这种拥有**宽护城河**的优秀企业，哪怕付出略高的市盈率也是值得的，因为时间会使其价值得到充分体现。他在买入后公开表示“我们预计将长期持有这些股票” 。事实证明，这笔投资极为成功：此后几十年可口可乐持续增长，伯克希尔从中获得了巨额的股息和资本增值。目前其持股市值已超过最初成本的20倍。可口可乐案例体现了价值投资的精髓——在 **“别人恐惧”** 时以合理价买入“护城河深厚”的伟业，并通过 **长期持有** 分享企业成长红利。\n\n​\t•\t**成功案例：苹果公司（Apple）** – 虽然巴菲特曾长期回避科技股，但2016年他开始建仓苹果公司，几年来投入逾300亿美元购入约5%的苹果股份。苹果之所以能打动这对投资搭档，正是因为它兼具**强大的护城河和卓越的管理**。一方面，苹果建立了独特的生态系统和品牌忠诚度。iPhone等产品在消费者心智中拥有不可取代的地位，用户粘性极高——巴菲特举例说：“如果你是苹果用户，有人给你一万美元但要求你此生不再买iPhone，你根本不会接受” 。相较之下，若有人出钱让你此生不买某品牌汽车，你可能拿钱转买竞争品牌。这个极端对比说明了苹果产品对消费者的价值是高度独占的，这正是苹果巨大的 **无形护城河**（品牌和生态）所在。另一方面，巴菲特对苹果管理层的执行力赞誉有加，称CEO蒂姆·库克“非常杰出，明白如何经营业务，把公司管理得无与伦比地好” 。库克接班后持续打造供应链优势，扩大服务业务，使苹果盈利和现金流再上台阶。在财务上，苹果拥有庞大的现金储备和惊人的盈利能力（ROE常年超过**40%**），几乎是巴菲特所见过“全世界最优秀的生意”之一。伯克希尔买入苹果后股价一路攀升，目前这项投资市值已超过1000亿美元，收益率惊人，同时苹果每年还向伯克希尔支付数十亿股息。苹果案例显示，即使是在科技行业，**只要公司拥有可持续竞争优势和一流管理**，价值投资的方法同样奏效。巴菲特把苹果视为伯克希尔的“第三大业务”，而非一只普通股票，可见其长线持有的决心。\n\n​\t•\t**失败案例：伯克希尔纺织业务** – 伯克希尔·哈撒韦公司本身最初是一家纺织厂，这是巴菲特早年在1960年代以“烟蒂股”思维收购的低价产业。当时纺织业已走向衰落，但他侥幸地认为凭借便宜的收购价和自己的管理改进或可扭转颓势。然而事实证明，纺织业务的**行业经济属性太差**，无论投入多少资金技改，都无法获得像样的回报。这成为巴菲特投资生涯中一个深刻的教训：他后来感叹“再好的骑手也无法让一匹跛脚的马赢得比赛”。正如他所总结的：“伯克希尔的纺织业务由诚实能干的人经营，但即便是这样的优秀管理层，在经济特性糟糕的行业中也寸步难行，如同在‘流沙’中奔跑，永远无法取得进展” 。最终，巴菲特在1985年不得不关闭了纺织厂业务，承认了这笔投资的失败。他事后反思道，如果当初把买纺织厂的钱直接投向股票市场或其他好企业，股东财富本可以增长得更多。纺织业案例教训在于：**没有护城河的夕阳行业，再便宜也不值得长期投资**。巴菲特从中学到要“远离坏生意”，日后他宁可以公允价买入伟大公司，也不愿贪便宜染指经济前景不佳的行业。\n\n​\t•\t**失败案例：航空公司投资** – 巴菲特对航空业的态度可谓一波三折，但总体而言这是令他 **“折戟”** 较多的领域。早在2007年致股东信中，他就将航空公司列为“糟糕行业”的典型代表之一，戏言道：“如果有远见的资本家在1903年莱特兄弟起飞时就在场，他本该一枪把他们打下来，这会让后来者省下无数财富” 。这番诙谐却辛辣的话揭示了航空业长期存在的问题：**资本投入大、竞争激烈、盈利薄弱**。几十年来，航空公司集体亏损累累，投资者投入的资金仿佛填不满无底洞。然而，巴菲特也曾两度涉足这一他所不看好的行业。一次是1989年他购买了USAir的优先股，当时因高息票面看似诱人，但不久公司业绩就急转直下，连利息都无法支付。巴菲特称这次投资是出于“一时愚蠢”，公司很快陷入财务困境，好在后来行业景气回升时他抓住机会在1998年卖出了持股，竟意外取得不错的回报。但事后证明这只是**侥幸逃脱**——USAir在他卖出后的十年间两度破产 。第二次是2016年左右，伯克希尔出人意料地买入了美国四大航空公司的股票（达美、西南、美国、联合），一度成为这些公司的主要股东。起初几年航空业盈利改善，巴菲特的持仓市值也上涨。然而2020年新冠疫情突袭使航空业遭受重创，需求断崖式下跌。巴菲特在2020年承认对行业前景判断失误，迅速以低价清仓了所有航空股，蒙受了数十亿美元的损失。这次教训再次印证了他先前的观点：航空业缺乏可靠的护城河，外部冲击下极易变为**惨烈的亏损陷阱**。总结航空业的失败案例，我们可以看到，即便是经验老到如巴菲特，有时也会违背自己“避免麻烦行业”的准则而付出代价。不过，他从不讳言错误，并将其转化为日后决策的宝贵经验：正如他说的，“投资最重要的就是不断学习”，哪怕向市场 **昂贵地缴学费** 也要汲取教训，避免重蹈覆辙。\n\n\n\n综上，沃伦·巴菲特和查理·芒格的投资思想体系严谨而完整：从坚持价值为本的 **核心哲学**，到苛刻挑选 **优秀而有护城河的公司**，以合理价格逆势 **买入**，然后耐心长期 **持有** 并回避市场噪音，最后通过正反两方面的 **案例** 总结验证其策略。这套投资逻辑框架强调理性、耐心和纪律，看似简单朴素，却在几十年的实践中助力他们穿越牛熊、创造了惊人的复利神话。对于广大投资者而言，巴菲特和芒格的智慧箴言提供了可靠的指南：**以企业主视角看待投资，专注价值而非价格波动；在具备优势的领域耐心等待机会，以安全边际买入优秀公司；买入后如无重大变故则坚定持有，让时间和复利为你发挥魔力。**这些理念跨越时代而历久弥新，正如巴菲特本人就是最好的例证——他始终坚守这些原则，并乐于将之言传身教，供后来者参考借鉴。  ","tags":["note"],"categories":["投资"]},{"title":"机器学习+深度学习量化","url":"/2025/03/08/机器学习+深度学习量化/","content":"\n### 背景\n最近在港股美股上的收益还可以，但是在择时能力上感觉靠人的判断没有特别的准确，希望通过量化的方式去帮助择时，也不希望做高频，只希望高抛低吸做t的时候可以更准确\n\n### 调研\n一开始尝试了bigquant，发现需要开会员才能用到比较高级的因子和策略，所以找了下开源的量化回测框架和数据\n目前采用的是以下几个库：\n1. [talib](https://github.com/TA-Lib/ta-lib-python:)\n2. [backtrader](https://github.com/mementum/backtrader)\n3. [akshare](https://github.com/akfamily/akshare)\n4. [pybroker](https://github.com/edtechre/pybroker)\n\n目前做到的是用机器学习和深度学习来做未来涨跌的判定，dnn/gnn的情况还在研究，因为构造数据集和回测框架的兼容上比机器学习难度大一些；\n\n### 效果\n特征粗略制作，样本就挑选了一个a股为例\n#### 交易图表：\n![汇总结论](https://github.com/miaohancheng/picx-images-hosting/raw/master/pics/backtrader_plot.7axbcmuk0o.webp)\n\n#### 执行过程\n1. 模型训练+分步减仓\n![减仓](https://github.com/miaohancheng/picx-images-hosting/raw/master/pics/执行过程.7snd18d7o4.webp)\n2. 模型训练+止损\n![止损](https://github.com/miaohancheng/picx-images-hosting/raw/master/pics/止损.26lmndbwuu.webp)\n\n#### 数据表现：\n- 最终资金: 1,483,439.88 CNY （初始资金：1,000,000.00 CNY）\n- 回测时间：2018-01-01 至 2025-03-01 的 大A\n- 年化夏普比率: 0.51\n- 最大回撤: 30.80% (持续 489 天)\n- 年化收益率: 5.90%（只能跑赢通胀）\n\n### 代码思路：\n初始化引入：\n```python\nimport os\nimport warnings\nimport logging\nimport akshare as ak\nimport backtrader as bt\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport talib as ta\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import StandardScaler\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n\n# ----------------------- 日志设置与警告屏蔽 -----------------------\nDEBUG = False\nif DEBUG:\n    logging.basicConfig(level=logging.DEBUG, format='%(asctime)s %(levelname)s: %(message)s')\nelse:\n    logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s')\n\nlogger = logging.getLogger(__name__)\n\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n```\n\n\n#### 模型构建:\n1. transfromer\n```python\n## ----------------------- Transformer模型定义 -----------------------\nclass TransformerClassifier(nn.Module):\n    def __init__(self, input_dim, nhead=2, num_encoder_layers=2, dim_feedforward=64, dropout=0.1):\n        super(TransformerClassifier, self).__init__()\n        # 如果 input_dim 不能被 nhead 整除，则增加一个线性层映射到新维度\n        if input_dim % nhead != 0:\n            new_input_dim = ((input_dim // nhead) + 1) * nhead\n            self.input_proj = nn.Linear(input_dim, new_input_dim)\n            d_model = new_input_dim\n        else:\n            self.input_proj = None\n            d_model = input_dim\n\n        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,\n                                                   dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n        # 分类层\n        self.fc = nn.Linear(d_model, 1)\n\n    def forward(self, x):\n        # x: (batch_size, seq_length, input_dim)\n        if self.input_proj is not None:\n            x = self.input_proj(x)\n        transformer_out = self.transformer_encoder(x)  # (batch_size, seq_length, d_model)\n        pooled = transformer_out.mean(dim=1)             # (batch_size, d_model)\n        output = self.fc(pooled)\n        return torch.sigmoid(output)\n\n```\n\n2. Bilstm+Attention\n```python\n\n# ------------------ 定义 BiLSTM+Attention 模型 ------------------\nclass BiLSTMAttention(nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_layers=1):\n        super(BiLSTMAttention, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True)\n        # 注意力层：将每个时刻的输出映射为一个得分\n        self.attn_layer = nn.Linear(hidden_dim * 2, 1)\n        self.fc = nn.Linear(hidden_dim * 2, 1)\n\n    def forward(self, x):\n        # x: (batch_size, seq_length, input_dim)\n        lstm_out, _ = self.lstm(x)  # (batch_size, seq_length, hidden_dim*2)\n        attn_scores = self.attn_layer(lstm_out)  # (batch_size, seq_length, 1)\n        attn_weights = torch.softmax(attn_scores, dim=1)  # (batch_size, seq_length, 1)\n        context = torch.sum(attn_weights * lstm_out, dim=1)  # (batch_size, hidden_dim*2)\n        output = self.fc(context)  # (batch_size, 1)\n        output = torch.sigmoid(output)\n        return output\n```\n\n#### 模型训练\n1. transformer\n```python\n\ndef train_transformer_model(X_train_seq, y_train_seq, X_val_seq, y_val_seq, input_dim,\n                            nhead=2, num_encoder_layers=2, dim_feedforward=64, dropout=0.1,\n                            epochs=20, batch_size=32, patience=5):\n    X_train_tensor = torch.tensor(X_train_seq, dtype=torch.float32)\n    y_train_tensor = torch.tensor(y_train_seq, dtype=torch.float32).unsqueeze(1)\n    X_val_tensor = torch.tensor(X_val_seq, dtype=torch.float32)\n    y_val_tensor = torch.tensor(y_val_seq, dtype=torch.float32).unsqueeze(1)\n\n    model = TransformerClassifier(input_dim, nhead, num_encoder_layers, dim_feedforward, dropout)\n    criterion = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    best_val_loss = float('inf')\n    epochs_no_improve = 0\n    best_model_state = None\n    for epoch in range(epochs):\n        model.train()\n        permutation = torch.randperm(X_train_tensor.size(0))\n        train_loss = 0\n        for i in range(0, X_train_tensor.size(0), batch_size):\n            optimizer.zero_grad()\n            indices = permutation[i:i+batch_size]\n            batch_x = X_train_tensor[indices]\n            batch_y = y_train_tensor[indices]\n            outputs = model(batch_x)\n            loss = criterion(outputs, batch_y)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item() * batch_x.size(0)\n        train_loss /= X_train_tensor.size(0)\n\n        model.eval()\n        with torch.no_grad():\n            val_outputs = model(X_val_tensor)\n            val_loss = criterion(val_outputs, y_val_tensor).item()\n\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            best_model_state = model.state_dict()\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n            if epochs_no_improve >= patience:\n                break\n    if best_model_state is not None:\n        model.load_state_dict(best_model_state)\n    model.eval()\n    with torch.no_grad():\n        val_preds = model(X_val_tensor)\n        val_preds_binary = (val_preds >= 0.5).float()\n        accuracy = (val_preds_binary.eq(y_val_tensor).sum().item() / y_val_tensor.size(0))\n    return model, accuracy\n\n```\n\n2. BiLSTM+Attention\n```python\ndef train_bilstm_attention_model(X_train_seq, y_train_seq, X_val_seq, y_val_seq, input_dim,\n                                 hidden_dim=32, epochs=20, batch_size=32, patience=5):\n    X_train_tensor = torch.tensor(X_train_seq, dtype=torch.float32)\n    y_train_tensor = torch.tensor(y_train_seq, dtype=torch.float32).unsqueeze(1)\n    X_val_tensor = torch.tensor(X_val_seq, dtype=torch.float32)\n    y_val_tensor = torch.tensor(y_val_seq, dtype=torch.float32).unsqueeze(1)\n\n    model = BiLSTMAttention(input_dim, hidden_dim)\n    criterion = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    best_val_loss = float('inf')\n    epochs_no_improve = 0\n    best_model_state = None\n    for epoch in range(epochs):\n        model.train()\n        permutation = torch.randperm(X_train_tensor.size(0))\n        train_loss = 0\n        for i in range(0, X_train_tensor.size(0), batch_size):\n            optimizer.zero_grad()\n            indices = permutation[i:i+batch_size]\n            batch_x = X_train_tensor[indices]\n            batch_y = y_train_tensor[indices]\n            outputs = model(batch_x)\n            loss = criterion(outputs, batch_y)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item() * batch_x.size(0)\n        train_loss /= X_train_tensor.size(0)\n\n        model.eval()\n        with torch.no_grad():\n            val_outputs = model(X_val_tensor)\n            val_loss = criterion(val_outputs, y_val_tensor).item()\n\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            best_model_state = model.state_dict()\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n            if epochs_no_improve >= patience:\n                break\n    if best_model_state is not None:\n        model.load_state_dict(best_model_state)\n    model.eval()\n    with torch.no_grad():\n        val_preds = model(X_val_tensor)\n        val_preds_binary = (val_preds >= 0.5).float()\n        accuracy = (val_preds_binary.eq(y_val_tensor).sum().item() / y_val_tensor.size(0))\n    return model, accuracy\n```\n\n3. 辅助函数\n```python\n# 辅助函数：构造序列数据\ndef create_sequences(X, y, seq_length):\n    X_seq, y_seq = [], []\n    for i in range(len(X) - seq_length + 1):\n        X_seq.append(X[i:i+seq_length])\n        y_seq.append(y[i+seq_length-1])\n    return np.array(X_seq), np.array(y_seq)\n```\n\n#### 特征工程：\n```python\n# ----------------------- 特征工程模块 -----------------------\ndef compute_features(close_list, volume_list, prediction_window):\n    \"\"\"\n    构造特征数据，返回DataFrame，其中：\n      - close_list: 按时间顺序排列的收盘价列表（最早在前）\n      - volume_list: 成交量列表\n      - prediction_window: 预测窗口（未来N日）\n    \"\"\"\n    df = pd.DataFrame({'close': close_list, 'volume': volume_list})\n    df['high'] = df['close']\n    df['low'] = df['close']\n\n    # 1. 滞后收益率\n    for window in [5, 10, 20]:\n        df[f'rtn_lag_{window}'] = df['close'].pct_change(window)\n\n    # 2. 日收益率 & 波动率\n    df['daily_return'] = df['close'].pct_change()\n    df['volatility_10'] = df['daily_return'].rolling(window=10).std()\n\n    # 3. 均线及差值\n    df['ma_20'] = df['close'].rolling(window=20).mean()\n    df['ma_diff'] = df['close'] / (df['ma_20'] + 1e-8) - 1\n\n    # 4. RSI（自定义与TA版本）\n    delta = df['close'].diff()\n    gain = delta.clip(lower=0)\n    loss = -delta.clip(upper=0)\n    avg_gain = gain.rolling(14).mean()\n    avg_loss = loss.rolling(14).mean()\n    rs = avg_gain / (avg_loss + 1e-8)\n    df['rsi_14_custom'] = 100 - (100 / (1 + rs))\n    df['ta_rsi'] = ta.RSI(df['close'].values, timeperiod=14)\n\n    # 5. MACD\n    df['ema_12'] = df['close'].ewm(span=12, adjust=False).mean()\n    df['ema_26'] = df['close'].ewm(span=26, adjust=False).mean()\n    df['macd_custom'] = df['ema_12'] - df['ema_26']\n    df['macd_signal_custom'] = df['macd_custom'].ewm(span=9, adjust=False).mean()\n    df['macd_hist_custom'] = df['macd_custom'] - df['macd_signal_custom']\n    df['ta_macd'], df['ta_macd_signal'], df['ta_macd_hist'] = ta.MACD(\n        df['close'].values, fastperiod=12, slowperiod=26, signalperiod=9)\n\n    # 6. Bollinger Bands\n    df['ta_boll_upper'], df['ta_boll_middle'], df['ta_boll_lower'] = ta.BBANDS(\n        df['close'].values, timeperiod=20)\n\n    # 7. ADX\n    df['ta_adx'] = ta.ADX(df['high'].values, df['low'].values, df['close'].values, timeperiod=14)\n\n    # 8. OBV\n    df['ta_obv'] = ta.OBV(df['close'].values, df['volume'].values)\n\n    # 9. 慢速KD\n    slowk, slowd = ta.STOCH(\n        df['high'].values, df['low'].values, df['close'].values,\n        fastk_period=14, slowk_period=3, slowk_matype=0,\n        slowd_period=3, slowd_matype=0\n    )\n    df['ta_stoch_slowk'] = slowk\n    df['ta_stoch_slowd'] = slowd\n\n    # 10. CCI\n    df['ta_cci'] = ta.CCI(df['high'].values, df['low'].values, df['close'].values, timeperiod=14)\n\n    # 11. Momentum\n    df['ta_mom'] = ta.MOM(df['close'].values, timeperiod=10)\n\n    # 12. ATR\n    df['ta_atr'] = ta.ATR(df['high'].values, df['low'].values, df['close'].values, timeperiod=14)\n\n    # 13. MFI\n    df['ta_mfi'] = ta.MFI(df['high'].values, df['low'].values, df['close'].values,\n                          df['volume'].values, timeperiod=14)\n\n    # 14. PPO\n    df['ta_ppo'] = ta.PPO(df['close'].values, fastperiod=12, slowperiod=26, matype=0)\n\n    # 15. ROC\n    df['ta_roc'] = ta.ROC(df['close'].values, timeperiod=10)\n\n    # 16. TRIX\n    df['ta_trix'] = ta.TRIX(df['close'].values, timeperiod=15)\n\n    # 17. Williams %R\n    df['ta_willr'] = ta.WILLR(df['high'].values, df['low'].values, df['close'].values, timeperiod=14)\n\n    # 18. StochRSI\n    df['ta_stochrsi'] = ta.STOCHRSI(\n        df['close'].values, timeperiod=14,\n        fastk_period=3, fastd_period=3, fastd_matype=0\n    )[0]\n\n    # 19. ULTOSC\n    df['ta_ultosc'] = ta.ULTOSC(\n        df['high'].values, df['low'].values, df['close'].values,\n        timeperiod1=7, timeperiod2=14, timeperiod3=28\n    )\n\n    # 20. Ichimoku\n    def compute_ichimoku(high, low):\n        high_series = pd.Series(high)\n        low_series = pd.Series(low)\n        tenkan = (high_series.rolling(window=9).max() + low_series.rolling(window=9).min()) / 2\n        kijun = (high_series.rolling(window=26).max() + low_series.rolling(window=26).min()) / 2\n        senkou_span_a = ((tenkan + kijun) / 2).shift(26)\n        senkou_span_b = (high_series.rolling(window=52).max() + low_series.rolling(window=52).min()) / 2\n        senkou_span_b = senkou_span_b.shift(26)\n        return tenkan, kijun, senkou_span_a, senkou_span_b\n\n    tenkan, kijun, senkou_a, senkou_b = compute_ichimoku(df['high'].values, df['low'].values)\n    df['ichi_tenkan'] = tenkan\n    df['ichi_kijun'] = kijun\n    df['ichi_senkou_a'] = senkou_a\n    df['ichi_senkou_b'] = senkou_b\n\n    # 21. 未来N日累积涨跌幅（预测目标）\n    df['cum_return_future'] = df['close'].pct_change(periods=prediction_window).shift(-prediction_window)\n\n    # 22. 52周range\n    df['52w_high'] = df['close'].rolling(window=252).max()\n    df['52w_low'] = df['close'].rolling(window=252).min()\n    df['range_52w'] = (df['close'] - df['52w_low']) / (df['52w_high'] - df['52w_low'] + 1e-8)\n\n    # 23. PE (示例)\n    df['pe'] = 15\n\n    # 24. 换手率\n    df['vol_ma_20'] = df['volume'].rolling(window=20).mean()\n    df['turnover'] = df['volume'] / (df['vol_ma_20'] + 1e-8)\n\n    # 标签：未来累计涨幅超过1%为正样本\n    df['label'] = (df['cum_return_future'] > 0.01).astype(int)\n\n    df = df.fillna(method='ffill').fillna(0)\n\n    feature_cols = [col for col in df.columns if col not in ['label', 'cum_return_future', 'daily_return']]\n    df = df[feature_cols + ['label', 'cum_return_future', 'daily_return']]\n    df = df.fillna(method='ffill').fillna(0)\n    return df, feature_cols\n\n\n```\n特征上用到都是比较基础统计类的特征，时序相关的还没开，实际上如果开发时序类特征，lstm+transformer的效果会好很多\n\n#### 主程序\n策略上就是止盈止损+一些回撤止盈，比较简单就不写了，然后下面是代码主入口\n```python\n\n# ----------------------- 主程序 -----------------------\nif __name__ == '__main__':\n    cerebro = bt.Cerebro()\n\n    symbol = '600519'\n    stock_data = fetch_akshare_data(\n        symbol=symbol,\n        start_date=pd.to_datetime('2018-01-01'),\n        end_date=pd.to_datetime('2025-03-01')\n    )\n\n    logger.info(\"\\n[数据样例]\")\n    logger.info(stock_data.head(3))\n    logger.info(f\"\\n数据时间范围: {stock_data.index.min()} ~ {stock_data.index.max()}\")\n\n    data = bt.feeds.PandasData(\n        dataname=stock_data,\n        open=0, high=1, low=2, close=3, volume=4, openinterest=-1\n    )\n    cerebro.adddata(data)\n\n    cerebro.addstrategy(\n        MLStrategy,\n        training_period=504,\n        prediction_window=3,\n        probability_threshold=0.65,\n    )\n\n    cerebro.broker.setcash(1_000_000)\n    cerebro.broker.setcommission(commission=0.001)\n\n    cerebro.addanalyzer(bt.analyzers.PyFolio, _name='pyfolio')\n    cerebro.addanalyzer(bt.analyzers.SharpeRatio, riskfreerate=0.0, annualize=True,\n                        timeframe=bt.TimeFrame.Days, _name='sharpe')\n    cerebro.addanalyzer(bt.analyzers.DrawDown, _name='drawdown')\n    cerebro.addanalyzer(bt.analyzers.TradeAnalyzer, _name='trade_analyzer')\n    cerebro.addanalyzer(bt.analyzers.TimeReturn, timeframe=bt.TimeFrame.Days, _name='time_return')\n\n    logger.info(\"\\n启动回测...\")\n    results = cerebro.run()\n    strat = results[0]\n\n    final_value = cerebro.broker.getvalue()\n    logger.info(f\"\\n最终资金: {final_value:,.2f} CNY\")\n\n    sharpe = strat.analyzers.sharpe.get_analysis()\n    if sharpe.get('sharperatio') is not None:\n        logger.info(f\"年化夏普比率: {sharpe['sharperatio']:.2f}\")\n    else:\n        logger.info(\"年化夏普比率: N/A\")\n\n    drawdown = strat.analyzers.drawdown.get_analysis()\n    logger.info(f\"最大回撤: {drawdown['max']['drawdown']:.2f}% (持续 {drawdown['max']['len']} 天)\")\n\n    trade_analyzer = strat.analyzers.trade_analyzer.get_analysis()\n    logger.info(\"\\n交易统计: %s\", trade_analyzer)\n\n    time_return = strat.analyzers.time_return.get_analysis()\n    if len(time_return) > 0:\n        daily_returns = pd.Series(time_return)\n        total_ret = (daily_returns + 1.0).prod() - 1.0\n        days = len(daily_returns)\n        annual_ret = (1 + total_ret)**(252/days) - 1 if days > 0 else None\n        if annual_ret is not None:\n            logger.info(f\"年化收益率: {annual_ret:.2%}\")\n        else:\n            logger.info(\"无法计算年化收益率\")\n    else:\n        logger.info(\"time_return结果为空，无法计算年化收益率\")\n\n    figs = cerebro.plot(style='candlestick', volume=False, barup='red', bardown='green')\n    if figs and figs[0]:\n        fig = figs[0][0]\n        fig.savefig(\"backtrader_plot.png\")\n        logger.info(\"图形已保存为 backtrader_plot.png\")\n    plt.show()\n```\n### 下一步计划\n1. 构造特征要加强，要筛选出更多的因子，这次只是实验，所以因子挑选的比较简单\n2. 模型结构可以调整一下，深度和注意力可以加多\n3. gnn再尝试一下，板块内部的股票实际上是有关联的\n\n当然这个代码非常粗糙，仅作为学习记录，如果有建议和不足请大家指正","tags":["deep learning","python","machine learning","quant trading"],"categories":["技术文章","投资"]},{"title":"美国绿卡流程记录","url":"/2024/12/01/美国绿卡流程记录/","content":"\n### 摘要\n\n主要介绍一下我在国内申请NIW，从开始准备材料到美国移民局通过审核的所有流程，现在排期还未排到，如果排到之后再更新面试、拿卡等流程。\n\n\n\n### 起因\n\n主要是因为covid-19的封控问题，导致想换个国家生活工作一段时间，但是已经错过了留学留在国外工作的时间，而美国公司直接招聘海外员工发放h1b的情况又比较少；\n\n其实可行的路就剩下两条：\n\n1. 国内直接找到可以rebase到海外的外企，比如google、amd这些\n2. 国内直接申请绿卡，拿到卡之后出去\n\n\n\n### 准备申请\n\n#### 前期调研\n\n首先查询了相关案例，发现有NIW、EB1A 两条路可以直接人在美国境外申请绿卡（无需雇主担保），两个要求、排期各不相同，两种类型具体介绍如下：\n\n- [NIW](https://zhuanlan.zhihu.com/p/652960567?zpf=1695417255872118784) (这个介绍也算是广告，大家自行甄别)\n\n- [EB1A](https://zhuanlan.zhihu.com/p/578761321)(这个介绍也算是广告，大家自行甄别)\n\n\n\n一开始是准备DIY，然后找了一个美国的移民律师咨询了NIW、EB1A等情况，咨询费200美元/小时；\n\n发现自己的情况在NIW的条件下勉强及格，EB1A的话目前还不达标，所以放弃了EB1A，专心搞NIW；\n\n\n\n#### 寻找中介\n\n我和律师发现自己申请材料的缺陷主要是两点，一个是美国相关的专业人士推荐信不够；另外是行业内的影响力不够，这点靠自己DIY比较困难，所以还是想了下决定找中介来做这个项目；\n\n知乎、搜索之类的加了好几个中介，发了多份材料之后，选定了一个中介来帮我找到律师写整个申请材料；\n\n其中自己要准备的材料非常多，下面列举一些：\n\n1. 薪资收入证明\n2. 在职证明\n3. 学历信息证明\n4. 专利信息\n5. 各种行业相关证书\n6. 新闻稿和发表论文等\n\n\n\n整体准备材料到律师完成，递交给美国移民局 耗时：10个月，其中因为疫情上海封控了一段时间，一般也是需要半年左右，很多材料可能手头都没有，还要去找和去学校、公司开具；\n\n\n\n### 移民局审核\n\n在律师完成所有的文件翻译、审核、校对之后，要求付清翻译费之类的费用，会直接帮我递交材料\n\n递交时间：2023-4\n\n<img src=\"https://github.com/miaohancheng/picx-images-hosting/raw/master/pics/niw-recipet.wilm8fufg.jpg\" alt=\"递件凭证\" style=\"zoom:33%;\" />\n\n\n\n中介反馈一般审批时间是半年左右\n\n等到了2023-9，收到了获批函，证明我的NIW申请第一步已经基本完成，后续等待排期；\n\n\n\n### 等待排期\n\n等待排期是比较漫长的，主要是没什么进展，一年可能就前进6个月左右；\n\n现在每晚1年申请绿卡，就要晚2年拿到。\n\n查看排期的比较好用的网站推荐：https://visa.careerengine.us/ （可以自己定好自己的pd）\n\n![排期表](https://github.com/miaohancheng/picx-images-hosting/raw/master/pics/排期表.73tzmehatm.jpg)\n\n\n\n\n\n### 待更新后续\n\n\n\n### 总结\n\n这个总体项目还是比较长的，建议大家如果有打算要尽早行动，早做准备。\n\n同时也是耗时耗精力，还会有很多意外、补充材料的情况。\n","tags":["NIW","美国绿卡"],"categories":["留学移民"]},{"title":"多语言NER模型微调","url":"/2024/11/29/多语言NER模型微调/","content":"\n\n\n### 背景\n\n工作中有遇到多语言的地址、短句等数据，需要标注出其中人名、快递公司、电话等信息，现有的开源数据集中这部分数据较少，主要问题是要自己构建对应的数据集、以及对于开源通过wiki训练的模型要尽可能保留训练的经验\n\n### 模型选取\n\n选择了RoBERTa基于 [**wikiann**](https://huggingface.co/datasets/unimelb-nlp/wikiann ) 预训练的模型 [**roberta-ner-multilingual**](https://huggingface.co/julian-schelb/roberta-ner-multilingual )，对于现实中的知识已经有一部分理解，但是对于地址等信息知识较少需要额外训练；\n\n最原始的基座模型是facebook的 [**xlm-roberta-large**](https://huggingface.co/FacebookAI/xlm-roberta-large)\n\n### 数据准备\n\n准备了训练和测试数据集放在了 `./data`目录下，数据大致格式如下\n\n```json\nJohn    B-PER\nlives   O\nin      O\nBerlin  B-LOC\n.      O\n\n他      O\n住      O\n在      O\n北京    B-LOC\n。      O\n```\n\n通过不同的标签定义不同的实体，在模型训练中可以通过单一语言的样本迁移一部分知识到其他语言，这也是多语言模型的一个优势。\n\n\n\n### 模型加载\n\n因为我在本机mac上训练，后续迁移到服务器上，所以写了判断设备的代码\n\n```python\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\nfrom transformers import DataCollatorForTokenClassification\nfrom datasets import Dataset, DatasetDict\nimport numpy as np\nimport evaluate\nfrom torch import nn\nimport torch.nn.functional as F\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# 确定设备\nif torch.cuda.is_available():\n    device = torch.device('cuda')\nelif torch.backends.mps.is_available():\n    device = torch.device('mps')\nelse:\n    device = torch.device('cpu')\n\n# 加载分词器和模型\ntokenizer = AutoTokenizer.from_pretrained(\"julian-schelb/roberta-ner-multilingual\", add_prefix_space=True)\nmodel = AutoModelForTokenClassification.from_pretrained(\"julian-schelb/roberta-ner-multilingual\")\nmodel.to(device)  # 将模型移动到设备\n\n```\n\n\n\n### 增加标签\n\n原有的模型中标签类别较少，如果有新的标签可以手动添加\n\n```python\n# 如果有新标签，更新标签列表和映射\noriginal_labels = list(model.config.id2label.values())\nnew_labels = ['B-FACILITY','I-FACILITY' ] # 加入你新的标签即可\nlabel_list = original_labels + new_labels\nlabel_to_id = {label: idx for idx, label in enumerate(label_list)}\nid_to_label = {idx: label for idx, label in enumerate(label_list)}\n```\n\n\n\n更新模型配置\n\n```python\n# 更新模型配置\nmodel.config.num_labels = len(label_list)\nmodel.config.id2label = id_to_label\nmodel.config.label2id = label_to_id\n\n# 替换分类层\nmodel.classifier = nn.Linear(model.config.hidden_size, model.config.num_labels)\n```\n\n\n\n### 教师模型\n\n希望模型在学习新知识的前提下，不要遗忘旧知识，所以再增加一个教师模型来做监督\n\n```python\n# 加载教师模型\nteacher_model = AutoModelForTokenClassification.from_pretrained(\"julian-schelb/roberta-ner-multilingual\")\nteacher_model.to(device)  # 将教师模型移动到设备\nteacher_model.eval()\n\n# 可选：冻结学生模型的部分层\nfor name, param in model.named_parameters():\n    if name.startswith('roberta.embeddings') or name.startswith('roberta.encoder.layer.0') or name.startswith('roberta.encoder.layer.1'):\n        param.requires_grad = False\n\n```\n\n\n\n### 加载数据\n\n```python\n# 定义读取数据的函数\ndef read_conll_data(filename):\n    tokens = []\n    ner_tags = []\n    with open(filename, 'r', encoding='utf-8') as f:\n        words = []\n        tags = []\n        for line in f:\n            line = line.strip()\n            if not line:\n                if words:\n                    tokens.append(words)\n                    ner_tags.append(tags)\n                    words = []\n                    tags = []\n            else:\n                splits = line.split()\n                if len(splits) >= 2:\n                    words.append(splits[0])\n                    tags.append(splits[1])\n        if words:\n            tokens.append(words)\n            ner_tags.append(tags)\n    return {'tokens': tokens, 'ner_tags': ner_tags}\n\n# 加载您的数据\ntrain_data = read_conll_data('./data/train.txt')\nvalidation_data = read_conll_data('./data/validation.txt')\n\ntrain_dataset = Dataset.from_dict(train_data)\nvalidation_dataset = Dataset.from_dict(validation_data)\n\n# 创建数据集对象\ndatasets = DatasetDict({'train': train_dataset, 'validation': validation_dataset})\n```\n\n\n\n\n\n### 数据预处理\n\n```python\n# 定义数据预处理函数\ndef tokenize_and_align_labels(examples):\n    tokenized_inputs = tokenizer(\n        examples['tokens'],\n        truncation=True,\n        is_split_into_words=True,\n        padding='longest',  # 启用填充\n        max_length=512,     # 可根据需要调整\n    )\n    labels = []\n    for i in range(len(tokenized_inputs['input_ids'])):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        label_ids = []\n        previous_word_idx = None\n        for word_idx in word_ids:\n            if word_idx is None:\n                label_ids.append(-100)\n            elif word_idx != previous_word_idx:\n                label_ids.append(label_to_id.get(examples['ner_tags'][i][word_idx], label_to_id['O']))\n            else:\n                label_ids.append(-100)\n            previous_word_idx = word_idx\n        labels.append(label_ids)\n    tokenized_inputs['labels'] = labels\n    return tokenized_inputs\n# 预处理数据\ntokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)\n\n```\n\n\n\n### 评估指标\n\n```python\n\n# 加载评估指标\nmetric = evaluate.load('seqeval')\n\n# 定义评估函数\ndef compute_metrics(p):\n\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n\n    true_labels = [\n        [label_list[l] for l in label if l != -100]\n        for label in labels\n    ]\n    true_predictions = [\n        [label_list[pred] for (pred, lab) in zip(prediction, label) if lab != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n\n    results = metric.compute(predictions=true_predictions, references=true_labels)\n    return {\n        'precision': results['overall_precision'],\n        'recall': results['overall_recall'],\n        'f1': results['overall_f1'],\n        'accuracy': results['overall_accuracy']\n    }\n\n# 定义 DataCollator\ndata_collator = DataCollatorForTokenClassification(\n    tokenizer,\n    padding='longest',\n    max_length=512,\n    return_tensors='pt'\n)\n\n```\n\n\n\n### 定义trainer\n\n因为要做知识蒸馏，所以需要自定义trainer，结合教师模型的结果来做损失判定\n\n```python\n\n# 定义自定义 Trainer，用于知识蒸馏\nclass DistillationTrainer(Trainer):\n    def __init__(self, teacher_model, temperature=2.0, alpha=0.5, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.teacher_model = teacher_model\n        self.temperature = temperature\n        self.alpha = alpha\n        self.teacher_model.eval()\n\n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        labels = inputs.pop(\"labels\").to(device)\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        outputs = model(**inputs)\n        student_logits = outputs.logits\n\n        # 计算学生模型的损失（交叉熵损失）\n        loss_fct = nn.CrossEntropyLoss()\n        active_loss = labels.view(-1) != -100\n        active_logits = student_logits.view(-1, self.model.config.num_labels)[active_loss]\n        active_labels = labels.view(-1)[active_loss]\n        student_loss = loss_fct(active_logits, active_labels)\n\n        with torch.no_grad():\n            teacher_outputs = self.teacher_model(**inputs)\n            teacher_logits = teacher_outputs.logits\n\n        # 提取学生模型中对应于原始标签的 logits\n        num_labels_teacher = teacher_logits.size(-1)  # 7\n        student_logits_for_kd = student_logits[:, :, :num_labels_teacher]\n\n        # 计算蒸馏损失（KL 散度）\n        loss_fct = nn.KLDivLoss(reduction='batchmean')\n        student_logits_temp = student_logits_for_kd / self.temperature\n        teacher_logits_temp = teacher_logits / self.temperature\n\n        distillation_loss = loss_fct(\n            F.log_softmax(student_logits_temp, dim=-1),\n            F.softmax(teacher_logits_temp, dim=-1)\n        ) * (self.temperature ** 2)\n        print(\"Student logits shape:\", student_logits.shape)\n        print(\"Teacher logits shape:\", teacher_logits.shape)\n        print(\"Student logits for KD shape:\", student_logits_for_kd.shape)\n        # 合并损失\n        loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n\n        return (loss, outputs) if return_outputs else loss\n\n# 定义训练参数，使用较小的学习率\n# 定义训练参数\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    logging_dir='./logs',\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=3e-5,\n    dataloader_pin_memory=False,\n)\n\n# 初始化自定义 Trainer\ntrainer = DistillationTrainer(\n    teacher_model=teacher_model,\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets['train'],\n    eval_dataset=tokenized_datasets['validation'],\n    compute_metrics=compute_metrics,\n    data_collator=data_collator,\n)\n\n# 开始训练\ntrainer.train()\n```\n\n\n\n\n\n### 最后保存模型\n\n``` python\n# 保存模型和分词器\ntrainer.save_model('./my_trained_model')\ntokenizer.save_pretrained('./my_trained_model')\n```\n\n\n\n\n\n### 结论\n\n目前测试下来，如果整体数据集都是全新的数据，增加教师模型对于模型训练帮助不大，不如直接开始微调基座模型，因为标签和数据都是未曾见过的，教师模型无法给出建议，只会有干扰，但是这对于新数据和旧数据有一些重叠的情况会有所帮助。\n","tags":["deep learning","python","ner"],"categories":["技术文章"]}]